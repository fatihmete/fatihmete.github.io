{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Big Data on Kubernetes","text":""},{"location":"#about-this-document","title":"About This Document","text":"<p>This documentation is a collection of notes I compiled while learning to deploy big data services on Kubernetes.</p>"},{"location":"#big-data-services","title":"Big Data Services","text":"<p>There are numerous big data tools available, but I selected one for each type of workload. For example, for a SQL engine, I chose Trino, and for a table catalog, I selected Apache Hive. My goal was to focus on widely used services. The targeted services covered in this documentation are listed below:</p> <ul> <li>Kubernetes (RKE2) / Custom Docker Registry</li> <li>HDFS (Storage)</li> <li>Apache Hive 4 (Metastore/Catalog)</li> <li>Apache Ranger (Trino, HDFS, Spark and Hive Policy Management/Audit) </li> <li>Trino (SQL Engine)</li> <li>Hue (Web-Based SQL Editor)</li> <li>Spark Notebook\u200a-\u200aJupyter Hub / Enterprise Gateway (Notebook)</li> <li>Iceberg (Table Format)</li> <li>Kerberos and LDAP Integration</li> </ul> <p>The following services are used in the documentation but are not covered in detail:</p> <ul> <li>Kerberos/LDAP Server: Refer to this guide and this documentation for setup instructions.</li> <li>PostgreSQL Database Server: Backend database for services like Ranger, Hive, and Hue. It is assumed that the database server is hosted outside the Kubernetes cluster.</li> <li>Solr: Used only for Ranger audit logs. GitHub Repository</li> <li>Longhorn Storage: Installation Guide</li> </ul>"},{"location":"#environment","title":"Environment","text":"<p>Initially, I experimented with tools like KIND and Minikube. However, I found these tools more suitable for quick tests or single-service development rather than a full-stack big data environment. A full-stack setup requires extensive configuration for networking, storage, and container deployment. As a result, I opted to use virtual machines, which also provide a more realistic, production-like experience.</p> <p>The virtual machines and host system used for this setup are as follows:</p> <ul> <li>Host: Desktop PC with 12 CPUs, 48GB RAM, and 2x480GB SATA SSDs running Arch Linux.</li> <li>Virtual Machines:</li> <li>1 x Kubernetes Master (Rocky Linux 9.5): 4 CPUs, 6GB RAM, 60GB Disk</li> <li>4 x Kubernetes Workers (Rocky Linux 9.5): 4 CPUs, 6GB RAM, 60GB Disk each</li> <li>1 x LDAP/KDC Server (Ubuntu Server 24.04): 2 CPUs, 2GB RAM, 20GB Disk</li> <li>1 x PostgreSQL Server (Rocky Linux 9.5): 2 CPUs, 2GB RAM, 20GB Disk</li> </ul> <p>For virtualization, I used the libvirt and QEMU stack.</p> <p>Although Kubernetes installation is not the primary focus of this documentation, I included a section on it for completeness. Rancher tools (e.g., RKE2, Rancher UI) simplify the process and provide a robust Kubernetes environment.</p>"},{"location":"#limitations","title":"Limitations","text":"<p>I am still in the process of learning the services covered in this documentation. Therefore, the content may not be production-ready or fully optimized.</p> <p>In most cases, I used NodePort services to expose applications outside the Kubernetes cluster. However, for production environments, you should consider using Ingress controllers, LoadBalancers, or high-availability tools such as HAProxy, Keepalived, MetalLB, or kube-vip.</p> <p>For persistent storage, I primarily used <code>hostPath</code> to keep the Kubernetes environment minimal. In production, you should consider using a CSI driver for persistent storage, such as <code>Longhorn</code>, <code>rook.io</code>, or <code>vSphere</code>.</p>"},{"location":"hdfs/","title":"HDFS","text":""},{"location":"hdfs/#foreword","title":"Foreword","text":"<p>Deploying HDFS on Kubernetes is a complex task. Fortunately, Apache provides official Docker images on Docker Hub, which serve as a good starting point. However, the critical decision lies in selecting the appropriate Kubernetes storage type, as HDFS requires persistent data storage.</p> <p>In a traditional HDFS setup, data is stored on disks in dedicated servers. Kubernetes, however, emphasizes resource abstraction, meaning resources should not be tied to specific nodes. This requires the use of persistent volumes. A suitable CSI (Container Storage Interface) driver is essential, and the choice depends on factors such as network speed, environment, data replication (for the CSI driver), and HDFS replication. While it is technically possible to deploy HDFS in the traditional way on Kubernetes, it may not align with Kubernetes' design principles.</p> <p>HDFS is highly fault-tolerant, protecting against data loss even if some nodes fail\u2014similar to Kubernetes. These similarities allow for multiple architectural approaches to deploying HDFS on Kubernetes. Below are three common methods:</p> <ol> <li>Using Persistent Volumes and StatefulSets: This approach uses Persistent Volumes for storage and StatefulSets to manage the NameNode and DataNodes. It is flexible and aligns well with Kubernetes' design principles.</li> <li>Using Host Disks with Node Affinity: This method uses host disks for storage and ties Kubernetes Pods to specific nodes using selectors. It is a more traditional, node-dependent setup.</li> <li>Using StatefulSets for the NameNode and DaemonSets for the DataNodes: This hybrid approach combines StatefulSets and DaemonSets for different HDFS components.</li> </ol> <p>...</p> <p>Each method has its trade-offs. For my home-level setup, the second method (host disks with node affinity) seemed more suitable. Kubernetes ensures high availability at the container level, while HDFS handles data protection against hardware failures. Additionally, learning through Kubernetes Pods as atomic units makes the concepts clearer and more practical.</p> <p>Tip</p> <p>Depending on your architecture, you might consider using external cloud object storage (e.g., S3, Azure Blob) or deploying HDFS or MinIO outside the Kubernetes cluster.</p> <p>In this documentation, I will demonstrate a basic HDFS setup consisting of a single NameNode and multiple DataNodes. The setup will use three nodes: <code>kube1</code>, <code>kube2</code>, and <code>kube3</code> as DataNodes, with <code>kube1</code> also serving as the NameNode.</p> <p>For simplicity, I will not deploy additional components such as ResourceManager (YARN), a secondary NameNode, JournalNode, Failover Controller, or HDFS Federation. YARN is unnecessary because we will deploy Spark on Kubernetes and use Trino instead.</p>"},{"location":"hdfs/#namespace","title":"Namespace","text":"<p>The first step is to create a namespace for the big data services:</p> <pre><code>kubectl create namespace bigdata\n</code></pre>"},{"location":"hdfs/#namenode","title":"Namenode","text":"<p>HDFS follows a master/slave architecture. An HDFS cluster consists of a single NameNode, which acts as the master server, managing the file system namespace and regulating client access to files.</p> <p>The NameNode requires persistent storage to maintain metadata about the file system. First, create the necessary directory on the node (in this case, <code>kube1</code>) and set the correct ownership:</p> <pre><code>mkdir -p /hadoop/nn\nchown -R 1000:1000 /hadoop/nn\n</code></pre> <p>Danger</p> <p>Ensure that ownership and permissions are set correctly. Most startup issues are caused by permission errors.</p> <p>The structure of the main development folder is as follows:</p> <pre><code>.\n\u251c\u2500\u2500 configs\n\u2502   \u251c\u2500\u2500 core-site.xml\n\u2502   \u2514\u2500\u2500 hdfs-site.xml\n\u2514\u2500\u2500 namenode.yaml\n</code></pre> <p>The YAML file for the NameNode Pod and its associated services is shown below:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: namenode\n  namespace: bigdata\n  labels:\n    app: namenode\n    dns: hdfs-subdomain\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: \"kube1\"\n  hostname: namenode\n  subdomain: company\n  containers:\n    - name: namenode\n      image: apache/hadoop:3.4.1\n      command: [\"/bin/bash\", \"-c\"]\n      args:\n        - |\n          if [ ! -d \"/hadoop/nn/current\" ]; then\n              hdfs namenode -format -force\n          fi\n          hdfs namenode\n      resources:\n        limits:\n          memory: \"1G\"\n          cpu: \"500m\"\n      volumeMounts:\n        - name: hadoop-config\n          mountPath: /opt/hadoop/etc/hadoop/core-site.xml\n          subPath: core-site.xml\n        - name: hadoop-config\n          mountPath: /opt/hadoop/etc/hadoop/hdfs-site.xml\n          subPath: hdfs-site.xml\n        - name: namenode-path\n          mountPath: /hadoop/nn\n  volumes:\n    - name: namenode-path\n      hostPath:\n        path: /hadoop/nn\n        type: Directory\n    - name: hadoop-config\n      configMap:\n        name: hadoop-config\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: company\n  namespace: bigdata\nspec:\n  selector:\n    dns: hdfs-subdomain\n  clusterIP: None\n  ports:\n    - name: rpc\n      port: 9000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: namenode-np\n  namespace: bigdata\nspec:\n  type: NodePort\n  selector:\n    app: namenode\n  ports:\n    - name: namenode-ui\n      port: 50470\n      targetPort: 50470\n      nodePort: 30570\n</code></pre> <p>We use the latest Docker image, apache/hadoop:3.4.1. To ensure the NameNode runs on the <code>kube1</code> node, we configure a node selector:</p> <pre><code>nodeSelector:\n  kubernetes.io/hostname: \"kube1\"\n</code></pre> <p>To maintain consistent hostnames (FQDNs) for Kerberos principals, we leverage Kubernetes' DNS service. By setting the hostname and subdomain, Kubernetes assigns the FQDN as <code>namenode.company.bigdata.svc.cluster.local</code>.</p> <ul> <li>Kubernetes DNS Documentation</li> </ul> <p>Tip</p> <p>If a headless Service exists in the same namespace as the Pod, with the same name as the subdomain, the cluster's DNS server will also return A and/or AAAA records for the Pod's fully qualified hostname. <pre><code>hostname: namenode\nsubdomain: company\n</code></pre></p> <p>When initializing HDFS for the first time, it must be formatted. This involves setting up a new distributed filesystem. We include the following command in the Pod's startup process to ensure the mounted path is formatted if <code>/hadoop/nn/current</code> does not exist. Finally, the NameNode is started:</p> <pre><code>command: [\"/bin/bash\", \"-c\"]\nargs:\n- |\n  if [ ! -d \"/hadoop/nn/current\" ]; then\n      hdfs namenode -format -force\n  fi\n  hdfs namenode\n</code></pre> <p>The host path <code>/hadoop/nn</code> is mounted from the node, and the required Hadoop configurations are retrieved from the ConfigMap:</p> <pre><code>volumes:\n  - name: namenode-path\n    hostPath:\n      path: /hadoop/nn\n      type: Directory\n  - name: hadoop-config\n    configMap:\n      name: hadoop-config\n</code></pre> <p>To manage HDFS configurations, two files are required: <code>core-site.xml</code> and <code>hdfs-site.xml</code>.</p> <ul> <li>Default core-site.xml: Hadoop Core Default Config</li> <li>Default hdfs-site.xml: Hadoop HDFS Default Config</li> </ul> <p>The following configurations are essential for deploying the NameNode. You can find the <code>core-site.xml</code> and <code>hdfs-site.xml</code> files in my GitHub repository:</p>"},{"location":"hdfs/#configscore-sitexml","title":"<code>configs/core-site.xml</code>","text":"<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://namenode.company.bigdata.svc.cluster.local:9000&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"hdfs/#configshdfs-sitexml","title":"<code>configs/hdfs-site.xml</code>","text":"<pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n        &lt;value&gt;/hadoop/nn&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.webhdfs.enable&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.http.address&lt;/name&gt;\n        &lt;value&gt;0.0.0.0:50470&lt;/value&gt;\n        &lt;final&gt;true&lt;/final&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>We use a ConfigMap to distribute the HDFS configuration files to the Pods:</p> <pre><code>kubectl create configmap hadoop-config -n bigdata --from-file=core-site.xml=./configs/core-site.xml --from-file=hdfs-site.xml=./configs/hdfs-site.xml\n</code></pre> <p>A headless Service is created with the same name as the subdomain. Kubernetes will generate A/AAAA records for this Service. To utilize this headless Service, the <code>dns: hdfs-subdomain</code> label must be added to the relevant object:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: company\n  namespace: bigdata\nspec:\n  selector:\n    dns: hdfs-subdomain\n  clusterIP: None\n  ports:\n    - name: rpc\n      port: 9000\n</code></pre> <p>Finally, we create a NodePort Service to access the Web UI:</p> <pre><code>ports:\n  - name: namenode-ui\n    port: 50070\n    targetPort: 50070\n    nodePort: 30570\n</code></pre> <p>To start the NameNode, run the following command:</p> <pre><code>kubectl apply -f namenode.yaml\n</code></pre> <p>If everything is configured correctly, you can access the NameNode Web UI at http://dns_or_ip_of_any_k8s_node:30570:</p> <p></p>"},{"location":"hdfs/#datanodes","title":"Datanodes","text":"<p>Datanodes requires persistent storage to store data. First, create the necessary path on data nodes and then set owner (in my case, <code>kube1</code>, <code>kube2</code>, <code>kube3</code>).</p> <pre><code>mkdir -p /hadoop/disk1\nchown -R 1000:1000 /hadoop/disk1\n</code></pre> <p>The YAML file for the first datanode:</p> datanode_01.yaml <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: datanode01\n  namespace: bigdata\n  labels:\n    app: datanode01\n    dns: hdfs-subdomain\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: \"kube1\"\n  hostname: datanode01\n  subdomain: company\n  containers:\n    - name: datanode01\n      image: apache/hadoop:3.4.1\n      command: [\"/bin/bash\", \"-c\"]\n      args:\n        - |\n          hdfs datanode\n      resources:\n        limits:\n          memory: \"512M\"\n          cpu: \"500m\"\n      volumeMounts:\n        - name: hadoop-config\n          mountPath: /opt/hadoop/etc/hadoop/core-site.xml\n          subPath: core-site.xml\n        - name: hadoop-config\n          mountPath: /opt/hadoop/etc/hadoop/hdfs-site.xml\n          subPath: hdfs-site.xml\n        - name: datanode-path\n          mountPath: /hadoop/disk1\n  volumes:\n    - name: datanode-path\n      hostPath:\n        path: /hadoop/disk1\n        type: Directory\n    - name: hadoop-config\n      configMap:\n        name: hadoop-config\n</code></pre> <p>It is similar to the namenode. We just change start script as hdfs datanode and mount <code>/hadoop/disk1</code> path. We need to add disk's location to <code>hdfs-site.xml</code> and update <code>hadoop-conf</code> ConfigMap. Since directories that not exist are ignored, we can add all possible disks here.</p> configs/hdfs-site.xml <pre><code>...\n&lt;property&gt;\n  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n  &lt;value&gt;/hadoop/disk1&lt;/value&gt;\n  &lt;description&gt;Determines where on the local filesystem an DFS data node\n      should store its blocks. If this is a comma-delimited\n      list of directories, then data will be stored in all named\n      directories, typically on different devices.\n      Directories that do not exist are ignored.\n  &lt;/description&gt;\n  &lt;final&gt;true&lt;/final&gt;\n&lt;/property&gt;\n...\n</code></pre> <pre><code>kubectl -n bigdata delete configmaps hadoop-config\nkubectl create configmap hadoop-config -n bigdata --from-file=core-site.xml=./configs/core-site.xml --from-file=hdfs-site.xml=./configs/hdfs-site.xml\nRedeploy namenode and deploy datanodes:\nkubectl delete -f namenode.yaml\nkubectl apply -f namenode.yaml\nkubectl apply -f datanode_01.yaml\nkubectl apply -f datanode_02.yaml\nkubectl apply -f datanode_03.yaml\n</code></pre> <p>Now we can see data nodes on namenode web UI:</p> <p></p> <p>Also now we can use hdfs cli commands. First we should access namenode bash:</p> <pre><code>kubectl -n bigdata exec -it namenode -- bash\n</code></pre> <p>You can execute following commands to test:</p> <pre><code>hdfs dfs -ls /\nhdfs dfs -mkdir /test\nhdfs dfs -touch /test/test_file\nhdfs dfs -ls /test\n</code></pre> <p>If you want to add more data node, you should change only the following parameters inside new data nodes <code>yaml</code> file:</p> <pre><code>....\nmetadata:\n  name: datanode03 #!!!\n  namespace: bigdata\n  labels:\n    app: datanode03 #!!!\n    dns: hdfs-subdomain\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: \"kube3\" #!!!\n  hostname: datanode03 #!!!\n  subdomain: company\n  containers:\n  - name: datanode03 #!!!\n    image: apache/hadoop:3.4.1\n...\n</code></pre> <p>Additionally you can use name node web UI (Utilities&gt;Browse the File System) to browse HDFS files.</p> <p></p>"},{"location":"hdfs/#links","title":"Links","text":"<ul> <li>https://hadoop.apache.org/docs/r3.4.1/hadoop-project-dist/hadoop-common/ClusterSetup.html</li> <li>https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html</li> <li>https://bytemedirk.medium.com/setting-up-an-hdfs-cluster-with-docker-compose-a-step-by-step-guide-4541cd15b168</li> <li>Default core-site.xml\u00a0: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml</li> <li>Default hdfs-site.xml: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</li> <li>https://gist.github.com/commandlineconsole/10e6a1cdf5702a7910ba70cf7b019f7f</li> </ul>"},{"location":"hive/","title":"Hive","text":"<p>Apache hive consist of two major component: HiveServer and Hive Metastore. HiveServer is sql engine which run on the hadoop ecosystem. HiveMetastore is a catalog which stores databases and tables information. </p>"},{"location":"hive/#hive-metastore","title":"Hive Metastore","text":"<p>Metastore needs a relational database backend. I used an external (outside of Kubernetes cluster) postgresql server. The official Hive docker image doesn't contain postgresql driver. To overcome this issue we need to build a custom hive image.</p>"},{"location":"hive/#docker-image","title":"Docker Image","text":"<p>First, download required driver. If you want to use different relational database you should download related driver. You can find postgre and mysql driver in my repository.</p> <pre><code>curl -fLo files/postgres.jar https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.4/postgresql-42.7.4.jar\n</code></pre> <p>Create a Dockerfile like following. </p> <p>Tip</p> <p><code>krb5-config</code> and <code>krb5-user</code> packages are optional. If you want to run kerberos client commands like <code>kinit</code>, <code>klist</code> inside the container, you should install these packages. Also we have created <code>hive</code> user home folder to avoid some errors.</p> Dockerfile <pre><code>FROM apache/hive:4.0.1\nCOPY files/postgres.jar /opt/hive/lib/postgres.jar\n# Mysql: COPY files/mysql.jar /opt/hive/lib/mysql.jar\n\nUSER root\n\nRUN apt-get update &amp;&amp; \\\nDEBIAN_FRONTEND=noninteractive apt-get -qq -y install krb5-config krb5-user\n\nRUN mkdir -p /home/hive\nRUN chown -R hive:hive /home/hive\nUSER hive\n</code></pre> <p>Then build and push it <pre><code>docker build -t kube5:30123/custom/hive:4.0.1 .\ndocker push kube5:30123/custom/hive:4.0.1\n</code></pre></p> <p>Tip</p> <p>You should use your own custom docker registry or docker hub account.</p>"},{"location":"hive/#kerberos-settings","title":"Kerberos Settings","text":"<p>Hive needs following principal for kerberos authentication:</p> <p><pre><code>hive/metastore.company.bigdata.svc.cluster.local@HOMELDAP.ORG\n</code></pre> You should create a keytab file for the principal. Then you should deploy it as Secret, you can use <code>ktutil</code> tool to create keytab. </p> <pre><code>kubectl create -n bigdata secret generic keytab-hive --from-file=./files/hive.keytab\n</code></pre>"},{"location":"hive/#hdfs-connection","title":"HDFS Connection","text":"<p>Create required HDFS directories: <pre><code>hdfs dfs -mkdir /tmp\nhdfs dfs -mkdir /warehouse\nhdfs dfs -mkdir /warehouse/external\nhdfs dfs -mkdir /warehouse/managed\nhdfs dfs -mkdir -p /opt/hive/scratch_dir # you can change this path in hive-site.xml\n</code></pre> Set permission for these directories</p> <pre><code>hdfs dfs -chmod 777 /tmp\nhdfs dfs -setfacl -R -m user:hive:rwx /warehouse\nhdfs dfs -setfacl -R -m user:hive:rwx /opt/hive/scratch_dir\n</code></pre> <p>Add following configurations to Hadoop <code>core-site.xml</code>: </p> <pre><code>...\n   &lt;property&gt;\n      &lt;name&gt;hadoop.proxyuser.hive.hosts&lt;/name&gt;\n      &lt;value&gt;*&lt;/value&gt;\n   &lt;/property&gt;\n   &lt;property&gt;\n      &lt;name&gt;hadoop.proxyuser.hive.groups&lt;/name&gt;\n      &lt;value&gt;*&lt;/value&gt;\n   &lt;/property&gt;\n   &lt;property&gt;\n      &lt;name&gt;hadoop.proxyuser.hive.users&lt;/name&gt;\n      &lt;value&gt;*&lt;/value&gt;\n   &lt;/property&gt;\n...\n</code></pre>"},{"location":"hive/#deployment","title":"Deployment","text":"<p>Tip</p> <p>As Hive Metastore is a stateless service, you can deploy it as a Deployment.</p>"},{"location":"hive/#configs","title":"Configs","text":"<p>Tip</p> <p>Hive server requires YARN kerberos principal to set. Otherwise It returns \"Can't get Master Kerberos principal for use as renewer\" error.</p> <p>https://stackoverflow.com/a/69068935/3648566</p> <p>https://steveloughran.gitbooks.io/kerberos_and_hadoop/content/sections/errors.html</p> configs/hive-site.xmlconfigs/yarn-site.xml <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.uris&lt;/name&gt;\n    &lt;value&gt;thrift://metastore.company.bigdata.svc.cluster.local:9083&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n    &lt;value&gt;/warehouse/managed&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.warehouse.external.dir&lt;/name&gt;\n    &lt;value&gt;/warehouse/external&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.event.db.notification.api.auth&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.try.direct.sql.ddl&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.try.direct.sql&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.dml.events&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.notification.add.thrift.object&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.server.filter.enabled&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.authorization.storage.checks&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;!-- Kerberos --&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.kerberos.keytab.file&lt;/name&gt;\n    &lt;value&gt;/etc/security/keytabs/hive.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hive/_HOST@HOMELDAP.ORG&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.server2.authentication&lt;/name&gt;\n    &lt;value&gt;KERBEROS&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;\n    &lt;value&gt;hive/_HOST@HOMELDAP.ORG&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;\n    &lt;value&gt;/etc/security/keytabs/hiveserver.keytab&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt;\n    &lt;value&gt;hdfs/_HOST@HOMELDAP.ORG&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Deploying configurations: <pre><code>kubectl create configmap hive-site-config -n bigdata --from-file=hive-site.xml=./configs/hive-site.xml --from-file=yarn-site.xml=./configs/yarn-site.xml\n</code></pre></p>"},{"location":"hive/#manifest-file","title":"Manifest file","text":"metastore.yaml <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: hive-metastore\n  namespace: bigdata\n  labels:\n    name: hive-metastore\n    app: hive-metastore\n    dns: hdfs-subdomain\nspec:\n  hostname: metastore\n  subdomain: company\n  hostAliases:\n  - ip: \"192.168.1.52\"\n    hostnames:\n    - \"kdc.homeldap.org\"\n  containers:\n  - name: hive-metastore\n    image: kube5:30123/custom/hive:4.0.1\n    imagePullPolicy: Always\n    env:\n    - name: SERVICE_OPTS\n      value: \"-Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://192.168.122.18:5432/hive -Djavax.jdo.option.ConnectionUserName=root -Djavax.jdo.option.ConnectionPassword=142536\"\n    - name: SERVICE_NAME\n      value: metastore\n    - name: DB_DRIVER\n      value: postgres\n    - name: VERBOSE\n      value: 'true'\n    - name: HIVE_CUSTOM_CONF_DIR\n      value: /hive-configs\n    resources:\n      limits:\n        memory: \"1G\"\n        cpu: \"1000m\"\n    ports:\n      - containerPort: 9083\n    volumeMounts:\n    - name: hive-site-config\n      mountPath: /hive-configs/hive-site.xml\n      subPath: hive-site.xml\n    - name: hive-site-config\n      mountPath: /hive-configs/yarn-site.xml\n      subPath: yarn-site.xml\n    - name: hadoop-config\n      mountPath: /hive-configs/core-site.xml\n      subPath: core-site.xml\n    - name: hadoop-config\n      mountPath: /hive-configs/hdfs-site.xml\n      subPath: hdfs-site.xml\n    - name: keytab-hive\n      mountPath: /etc/security/keytabs/\n    - name: krb5conf\n      mountPath: /etc/krb5.conf\n      subPath: krb5.conf\n  volumes:\n  - name: hive-site-config\n    configMap:\n      name: hive-site-config\n  - name: hadoop-config\n    configMap:\n      name: hadoop-config\n  - name: keytab-hive\n    secret:\n      secretName: keytab-hive\n  - name: krb5conf\n    configMap:\n      name: krb5conf\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hive-metastore\n  namespace: bigdata\nspec:\n  type: ClusterIP\n  selector:\n    app: hive-metastore\n  ports:\n  - port: 9083\n    targetPort: 9083\n</code></pre> <p>Ensure that use same headless service named same as subdomain value:</p> <pre><code>...\n  labels:\n    name: hive-metastore\n    app: hive-metastore\n    dns: hdfs-subdomain\nspec:\n  hostname: metastore\n  subdomain: company\n...\n</code></pre> <p>Set container image as our custom image that related sql driver included. Setting to <code>imagePullPolicy</code> as <code>Always</code> can be useful when testing the custom image. <pre><code>...\nimage: kube5:30123/custom/hive:4.0.1\nimagePullPolicy: Always\n...\n</code></pre> Official hive images takes some environment variables. To set postgresql (or mysql) configurations we should set <code>SERVICE_OPTS</code> variable. </p> <pre><code>- name: SERVICE_OPTS\n  value: \"-Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://&lt;database_host&gt;:&lt;database_port&gt;/&lt;database_name&gt; -Djavax.jdo.option.ConnectionUserName=&lt;database_user_name&gt; -Djavax.jdo.option.ConnectionPassword=&lt;database_password&gt;\"\n- name: DB_DRIVER\n  value: postgres\n</code></pre> <p>Tip</p> <p>We can set maximum heap memory using the <code>-Xmx1G</code> parameter.</p> <p>While the <code>SERVICE_NAME</code> variable determines which hive service will run, the <code>HIVE_CUSTOM_CONF_DIR</code> variable determines the location of the configuration files.</p> <pre><code>- name: SERVICE_NAME\n  value: metastore\n- name: HIVE_CUSTOM_CONF_DIR\n  value: /hive-configs\n</code></pre> <p>We mount Hive configurations, <code>krb5.conf</code> and hive keytab file. Also for HDFS support we should mount HDFS configuration files.  <pre><code>...\n    volumeMounts:\n    - name: hive-site-config\n      mountPath: /hive-configs/hive-site.xml\n      subPath: hive-site.xml\n    - name: hive-site-config\n      mountPath: /hive-configs/yarn-site.xml\n      subPath: yarn-site.xml\n    - name: hadoop-config\n      mountPath: /hive-configs/core-site.xml\n      subPath: core-site.xml\n    - name: hadoop-config\n      mountPath: /hive-configs/hdfs-site.xml\n      subPath: hdfs-site.xml\n    - name: keytab-hive\n      mountPath: /etc/security/keytabs/\n    - name: krb5conf\n      mountPath: /etc/krb5.conf\n      subPath: krb5.conf\n  volumes:\n  - name: hive-site-config\n    configMap:\n      name: hive-site-config\n  - name: hadoop-config\n    configMap:\n      name: hadoop-config\n  - name: keytab-hive\n    secret:\n      secretName: keytab-hive\n  - name: krb5conf\n    configMap:\n      name: krb5conf\n...\n</code></pre> Deploy hive metastore: <pre><code>kubectl apply -f metastore.yaml\n</code></pre></p>"},{"location":"hive/#hive-server","title":"Hive Server","text":"<p>I deployed minimal, not distributed, Hive Server. It is only to test and control metastore. </p> <p>Because we enable kerberos authentication for metastore, we need the following hive server keytab and kerberos settings:</p> <p><pre><code>hive/hiveserver.company.bigdata.svc.cluster.local@HOMELDAP.ORG\n</code></pre> Deploy the hive server keytab as secret: <pre><code>kubectl create -n bigdata secret generic keytab-hive-server --from-file=./files/hiveserver.keytab\n</code></pre></p> hiveserver.yaml <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: hive-server\n  namespace: bigdata\n  labels:\n    app: hive-server\n    dns: hdfs-subdomain\nspec:\n  hostname: hiveserver\n  subdomain: company\n  containers:\n  - name: hive-server\n    image: kube5:30123/custom/hive:4.0.1\n    imagePullPolicy: Always\n    env:\n    - name: SERVICE_NAME\n      value: hiveserver2\n    - name: HIVE_CUSTOM_CONF_DIR\n      value: /hive-configs\n    - name: SERVICE_OPTS\n      value: \"-Dhive.metastore.uris=thrift://metastore.company.bigdata.svc.cluster.local:9083\"\n    resources:\n      limits:\n        memory: \"2G\"\n        cpu: \"1000m\"\n    ports:\n      - containerPort: 10000\n      - containerPort: 10002\n    volumeMounts:\n    - name: hive-site-config\n      mountPath: /hive-configs/hive-site.xml\n      subPath: hive-site.xml\n    - name: hive-site-config\n      mountPath: /hive-configs/yarn-site.xml\n      subPath: yarn-site.xml\n    - name: hadoop-config\n      mountPath: /hive-configs/core-site.xml\n      subPath: core-site.xml\n    - name: hadoop-config\n      mountPath: /hive-configs/hdfs-site.xml\n      subPath: hdfs-site.xml\n    - name: keytab-hive-server\n      mountPath: /etc/security/keytabs/\n    - name: krb5conf\n      mountPath: /etc/krb5.conf\n      subPath: krb5.conf\n  volumes:\n  - name: hive-site-config\n    configMap:\n      name: hive-site-config\n  - name: hadoop-config\n    configMap:\n      name: hadoop-config\n  - name: keytab-hive-server\n    secret:\n      secretName: keytab-hive-server\n  - name: krb5conf\n    configMap:\n      name: krb5conf\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hive-server-svc\n  namespace: bigdata\nspec:\n  type: ClusterIP\n  selector:\n    app: hive-server\n  ports:\n  - port: 10000\n    targetPort: 10000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hive-server-np\n  namespace: bigdata\nspec:\n  type: NodePort\n  selector:\n    app: hive-server\n  ports:\n  - port: 10000\n    name: server\n    targetPort: 10000\n    nodePort: 31000\n  - port: 10002\n    name: webui\n    targetPort: 10002\n    nodePort: 31002\n</code></pre> <p>Set following parameters to run hive server: <pre><code>    - name: SERVICE_NAME\n      value: hiveserver2\n    - name: HIVE_CUSTOM_CONF_DIR\n      value: /hive-configs\n    - name: SERVICE_OPTS\n      value: \"-Dhive.metastore.uris=thrift://metastore.company.bigdata.svc.cluster.local:9083\"\n</code></pre> Additionaly we create a nodeport service, to access hive server web ui and hive server from outside of kubernetes cluster.</p> <p>Deploy hive server: <pre><code>kubectl apply -f hiveserver.yaml\n</code></pre></p> <p>To test our metastore first we should access hive server CLI:</p> <p><pre><code>kubectl -n bigdata exec -it hive-server -- bash\n</code></pre> Then we should take kerberos ticket and connect to hive server using <code>beeline</code>: </p> <p>kinit: <pre><code>kinit -kt /etc/security/keytabs/hiveserver.keytab hive/hiveserver.company.bigdata.svc.cluster.local@HOMELDAP.ORG\n</code></pre> beeline: <pre><code>beeline -u 'jdbc:hive2://localhost:10000/default;principal=hive/hiveserver.company.bigdata.svc.cluster.local@HOMELDAP.ORG'\n</code></pre> Now you can run sql commands on hive server:</p> <p></p> <p></p>"},{"location":"hue/","title":"Hue","text":"<p>Hue is a powerful tool for big data that allows users to run queries on different SQL engines and perform operations on HDFS directories. In addition to these capabilities, it offers many other features that enhance data exploration and management.  </p>"},{"location":"hue/#helm-chart","title":"HELM Chart","text":"<p>Fortunately, hue releases official helm charts.</p> <p>Download and extract the helm chart: <pre><code>helm pull gethue/hue\ntar -xvzf hue-1.0.3.tgz\nmv hue chart\n</code></pre></p>"},{"location":"hue/#kerberos","title":"Kerberos","text":"<p>We should create a principle for hue. Host FQDN is not required in principle. <pre><code>hue@HOMELDAP.ORG\n</code></pre></p> <p>Deploy as secret: <pre><code>kubectl create -n bigdata secret generic keytab-hue --from-file=./files/hue.keytab\n</code></pre></p>"},{"location":"hue/#configurations","title":"Configurations","text":"<p>We should create <code>values.yaml</code> to set helm chart configurations. Hue uses relational database. So we should set database parameters. Additionally, we set kerberos, HDFS, hive (beeswax) configurations.</p> values.yaml <pre><code>hue:\n  replicas: 1\n  database:\n    create: false\n    persist: false\n    engine: \"postgresql_psycopg2\"\n    host: \"192.168.122.18\" # Postgresql Host\n    port: 5432 # Postgresql Port\n    user: \"root\" # Postgresql User\n    password: \"142536\" # Postgresql Password\n    name: \"hue\" # You must create hue database before deployment\n  ini: |\n    [desktop]\n      ssl_validate=false\n      [[kerberos]]\n      hue_keytab=/etc/security/keytabs/hue.keytab\n      hue_principal=hue@HOMELDAP.ORG\n      ccache_path=/tmp/krb5cc_1001\n    [beeswax]\n      # Host where HiveServer2 is running.\n      # If Kerberos security is enabled, use fully-qualified domain name (FQDN).\n      hive_server_host=hiveserver.company.bigdata.svc.cluster.local\n\n      # Port where HiveServer2 Thrift server runs on.\n      hive_server_port=10000\n    [hadoop]\n      [[hdfs_clusters]]\n        [[[default]]]\n        fs_defaultfs=hdfs://namenode.company.bigdata.svc.cluster.local:9000\n        webhdfs_url=https://namenode.company.bigdata.svc.cluster.local:50470/webhdfs/v1\n        security_enabled=True\n        mechanism=GSSAPI\n</code></pre> <p>To mount kerberos related files we should do some modifications in helm chart.</p> chart/templates/deployment-hue.yaml <pre><code>...\n        volumeMounts:\n        - name: config-volume\n          mountPath: /usr/share/hue/desktop/conf/z-hue.ini\n          subPath: hue-ini\n        - name: config-volume-extra\n          mountPath: /usr/share/hue/desktop/conf/zz-hue.ini\n          subPath: hue-ini\n        # - name: hive-config-volume\n        #   mountPath: /etc/hive/conf/hive-site.xml\n        #   subPath: hive-site\n          ## KERBEROS\n        - name: hive-site-config\n          mountPath: /etc/hive/conf/hive-site.xml\n          subPath: hive-site.xml\n        - name: krb5conf\n          mountPath: /etc/krb5.conf\n          subPath: krb5.conf\n        - name: keytab-hue\n          mountPath: /etc/security/keytabs/\n        readinessProbe:\n...\n\n{{ end }}\n      volumes:\n        - name: config-volume\n          configMap:\n            name: hue-config\n        - name: config-volume-extra\n          configMap:\n            name: hue-config-extra\n        - name: hive-config-volume\n          configMap:\n            name: hive-config\n        ## KERBEROS\n        - name: hive-site-config\n          configMap:\n            name: hive-site-config\n        - name: keytab-hue\n          secret:\n            secretName: keytab-hue\n        - name: krb5conf\n          configMap:\n            name: krb5conf\n{{ if .Values.balancer.enabled }}\n...\n</code></pre> <p>Also we add kerberos and ldap DNS records:</p> chart/templates/deployment-hue.yaml <pre><code>...\n{{ end }}\n      hostAliases:\n      - ip: \"192.168.1.52\"\n        hostnames:\n        - \"kdc.homeldap.org\"\n        - \"homeldap.org\"\n      containers:\n      - name: hue\n        image: {{ .Values.image.registry }}/hue:{{ .Values.image.tag }}\n...\n</code></pre>"},{"location":"hue/#hadoop-proxy-userimpersonation","title":"Hadoop Proxy User/Impersonation","text":"<p>Proxy user configurations provides a superuser can submit jobs or access hdfs on behalf of another user. We should add following configuration  to Hadoop <code>core-site.xml</code>: </p> <pre><code>...\n   &lt;property&gt;\n      &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;\n      &lt;value&gt;*&lt;/value&gt;\n   &lt;/property&gt;\n   &lt;property&gt;\n      &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;\n      &lt;value&gt;*&lt;/value&gt;\n   &lt;/property&gt;\n   &lt;property&gt;\n      &lt;name&gt;hadoop.proxyuser.hue.users&lt;/name&gt;\n      &lt;value&gt;*&lt;/value&gt;\n   &lt;/property&gt;\n...\n</code></pre> <p>Tips</p> <ul> <li>After the changes you should update hadoop ConfigMap. </li> <li>proxyuser</li> </ul>"},{"location":"hue/#deploy-helm-chart","title":"Deploy Helm Chart","text":"<p>To install or update HELM chart run following commands: <pre><code>helm install hue ./chart -n bigdata -f values.yaml # Install\n\nhelm upgrade hue ./chart -n bigdata -f values.yaml # Update\n\nhelm uninstall hue -n bigdata # Uninstall\n</code></pre></p> <p>The command returns the following output: <pre><code>helm install hue ./chart -n bigdata -f values.yaml\nNAME: hue\nLAST DEPLOYED: Thu Mar 27 19:35:43 2025\nNAMESPACE: bigdata\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCongratulations, you've launched the Hue SQL Editor for Data Warehouses!\n\nTo check the status of your installation run:\n\n  helm list --filter hue\n\n\nGet the recommended URL below and start executing queries:\n\n\n  export WEB_HOST=$(kubectl get node -o jsonpath=\"{.items[0].metadata.name}\")\n\n  export WEB_PORT=$(kubectl get service hue-balancer -o jsonpath=\"{.spec.ports[*].nodePort}\" --namespace bigdata)\n\n  echo http://$WEB_HOST:$WEB_PORT\n</code></pre></p> <p>To access Hue Web UI URL, run following commands one by one: <pre><code>export WEB_HOST=$(kubectl get node -o jsonpath=\"{.items[0].metadata.name}\")\nexport WEB_PORT=$(kubectl get service hue-balancer -o jsonpath=\"{.spec.ports[*].nodePort}\" --namespace bigdata)\necho http://$WEB_HOST:$WEB_PORT\n</code></pre></p> <p>Alternatively, If you want to access hue always same port, you can create a node port service: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: hue-np\n  namespace: bigdata\nspec:\n  type: NodePort\n  selector:\n    pod: hue\n  ports:\n  - port: 8888\n    targetPort: 8888\n    nodePort: 30888\n</code></pre> Hue wants to set administrator account password when first login:</p> <p></p> <p>You can access HDFS files using Files button on the left menu:</p> <p>Tips</p> <p>Hue creates automatically user folder on HDFS <code>/user/</code> path</p> <p></p> <p>You can run hive queries using Hive editor:</p> <p></p>"},{"location":"hue/#ldap-login","title":"LDAP Login","text":"<p>To enable LDAP backend to login hue, you should add following settings to <code>values.yaml</code> below the <code>[desktop]</code> settings. </p> <pre><code>ini: |\n\n    [desktop]\n      ssl_validate=false\n      [[kerberos]]\n      hue_keytab=/etc/security/keytabs/hue.keytab\n      hue_principal=hue@HOMELDAP.ORG\n      ccache_path=/tmp/krb5cc_1001\n\n      [[auth]]\n      backend=desktop.auth.backend.LdapBackend, desktop.auth.backend.AllowFirstUserDjangoBackend\n\n      [[ldap]]\n      nt_domain=HOMELDAP.ORG\n      ldap_url=ldap://HOMELDAP.ORG:389\n      base_dn=\"ou=People,dc=homeldap,dc=org\"\n      bind_dn=\"uid=master,ou=People,dc=homeldap,dc=org\"\n      search_bind_authentication=false\n      create_users_on_login=true\n\n      [[[users]]]\n      user_filter=\"objectclass=inetOrgPerson\"\n      user_name_attr=\"uid\"\n</code></pre> <p>Warning</p> <p>Most probably, your LDAP server configurations will be different from mine. You can follow Hue LDAP guide for correct configurations Hue ldap settings</p> <p>Then update HELM chart: <pre><code>helm upgrade hue ./chart -n bigdata -f values.yaml\n</code></pre> Hue adds LDAP option to login screen. Still you can use DjangoBackend users.</p> <p></p>"},{"location":"hue/#other-settings","title":"Other Settings","text":"<p>Other settings you can follow Hue server settings  and connector settings. </p>"},{"location":"hue/#links","title":"Links","text":"<ul> <li>https://docs.ezmeral.hpe.com/datafabric-customer-managed/73/Hue/ConfigureHuetouseKerberos.html</li> <li>https://docs.ezmeral.hpe.com/datafabric-customer-managed/79/Hue/EnableSSLEncryption-HueAndHttpFS.html</li> <li>values.yaml reference, https://github.com/cloudera/hue/blob/master/tools/kubernetes/helm/hue/values.yaml</li> </ul>"},{"location":"kubernetes/","title":"Kubernetes Installation","text":"<p>Warning</p> <p>This guide is intended for testing and learning purposes only. It omits many details necessary for production environments. Do not use this setup in production.</p>"},{"location":"kubernetes/#kubernetes-nodes","title":"Kubernetes Nodes","text":"<p>For this cluster, I used five virtual machines (VMs): one master node and four worker nodes. In production environments, at least three master nodes are recommended for high availability. The VMs can have different configurations or sizes. I installed Rocky Linux 9.5 on all VMs, but any compatible Linux distribution can be used.</p> <p>The host system runs Arch Linux with QEMU/KVM and Libvirt for virtualization. Below is an overview of the VMs: </p> <p>Before proceeding with the installation, ensure your setup meets all prerequisites outlined in the RKE2 Quick Start Guide.</p> <p>Rocky Linux 9.5 satisfies most prerequisites by default. However, I had to disable the <code>firewalld</code> service and SELinux on all nodes: <pre><code>systemctl disable --now firewalld\nsudo sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config &amp;&amp; sudo setenforce 0\n</code></pre></p> <p>Additionally, I updated the <code>/etc/hosts</code> file on all nodes and the host machine to simplify node communication. This step is optional but recommended. Below is an example of my <code>/etc/hosts</code> file: <pre><code>192.168.1.61 kube1\n192.168.1.62 kube2\n192.168.1.63 kube3\n192.168.1.64 kube4\n192.168.1.65 kube5\n</code></pre></p>"},{"location":"kubernetes/#rancher-kubernetes-engine-rke2","title":"Rancher Kubernetes Engine (RKE2)","text":"<p>RKE2 is a production-grade Kubernetes distribution provided by Rancher. Also known as \"RKE Government,\" it is designed for enhanced security and compliance, particularly for U.S. Federal Government use cases. For more details, refer to the RKE2 documentation.</p>"},{"location":"kubernetes/#setting-up-the-first-master-node","title":"Setting Up the First Master Node","text":"<p>Master nodes run critical Kubernetes components such as etcd and the control plane. For high availability, at least three master nodes are recommended. However, for this testing setup, I used a single master node.</p> <p>RKE2 provides an installation script that automatically checks your environment and installs the required packages.</p> <p>Tip</p> <p>All commands in this guide should be run as a superuser.</p> <p>To set up the first master node (e.g., <code>kube1</code>), download and execute the installation script: <pre><code>curl -sfL https://get.rke2.io | sh -\n</code></pre></p> <p>Start the <code>rke2-server</code> service: <pre><code>systemctl enable --now rke2-server.service\n</code></pre></p> <p>Verify the service status: <pre><code>systemctl status rke2-server.service\n</code></pre> </p>"},{"location":"kubernetes/#setting-up-additional-master-nodes-optional","title":"Setting Up Additional Master Nodes (Optional)","text":"<p>To add more master nodes, configure each new node to join the existing cluster. Before installation, create a configuration file on the new node (e.g., <code>kube2</code>): <pre><code>mkdir -p /etc/rancher/rke2/\nvim /etc/rancher/rke2/config.yaml\n</code></pre></p> <p>Add the following values to the <code>config.yaml</code> file: <pre><code>server: https://&lt;server&gt;:9345\ntoken: &lt;token from server node&gt;\n</code></pre> - <code>server</code>: The DNS name or IP address of the first master node. - <code>token</code>: The cluster token, which can be retrieved from the first master node: <pre><code>cat /var/lib/rancher/rke2/server/node-token\n</code></pre></p> <p>For example, my <code>config.yaml</code> file looks like this: <pre><code>server: https://kube1:9345\ntoken: K1010d01bdaf32a792d9d233981959a757addaa6e8f6d622a68948467e495144155::server:af29acb6148c5132be664524a8822a76\n</code></pre></p> <p>Next, install RKE2 on the new master node: <pre><code>curl -sfL https://get.rke2.io | sh -\n</code></pre></p> <p>Start the <code>rke2-server</code> service: <pre><code>systemctl enable --now rke2-server.service\n</code></pre></p> <p>After a few minutes, the new master node should join the cluster. Verify the cluster status using the following command on any master node: <pre><code>KUBECONFIG=/etc/rancher/rke2/rke2.yaml /var/lib/rancher/rke2/bin/kubectl get nodes\n</code></pre></p> <p>The output should display all nodes in the cluster: </p> <p>You can add more master nodes using the same steps. For more details on distributing Kubernetes roles, refer to the RKE2 server roles documentation.</p>"},{"location":"kubernetes/#worker-nodes","title":"Worker Nodes","text":"<p>The process for setting up worker nodes is similar to adding additional master nodes, with one key difference: you must specify the worker node type.</p> <p>First, create the <code>config.yaml</code> file on the worker node: <pre><code>mkdir -p /etc/rancher/rke2/\nvim /etc/rancher/rke2/config.yaml\n</code></pre></p> <p>Add the same <code>server</code> and <code>token</code> values as in the master node setup.</p> <p>To install the worker node, run the following command: <pre><code>curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=\"agent\" sh -\n</code></pre></p> <p>Start the <code>rke2-agent</code> service: <pre><code>systemctl enable --now rke2-agent.service\n</code></pre></p> <p>Verify the worker node status from any master node: <pre><code>KUBECONFIG=/etc/rancher/rke2/rke2.yaml /var/lib/rancher/rke2/bin/kubectl get nodes\n</code></pre></p> <p>Repeat these steps to add more worker nodes.</p>"},{"location":"kubernetes/#accessing-the-kubernetes-cluster","title":"Accessing the Kubernetes Cluster","text":"<p>Manually connecting to the master node for every Kubernetes CLI command can be inefficient. Instead, you can configure access to the cluster from your local machine.</p> <p>First, install <code>kubectl</code> on your local machine by following the official guide.</p> <p>Next, copy the <code>/etc/rancher/rke2/rke2.yaml</code> file from the master node to your local machine. Update the <code>server</code> address in the configuration file to the master node's IP address or DNS name (instead of <code>127.0.0.1</code> or <code>localhost</code>).</p> <p>For example: </p> <p>Warning</p> <p>Ensure that your local machine has network access to the master node. You may need to adjust firewall rules.</p> <p>After updating the configuration, verify the connection: <pre><code>kubectl get nodes\n</code></pre></p> <p>If successful, you should see a list of nodes in your cluster: </p>"},{"location":"kubernetes/#kubernetes-management","title":"Kubernetes Management","text":"<p>You can use various tools to manage your Kubernetes cluster. Some popular options include:</p> <ul> <li>Rancher UI docker</li> <li>Portainer</li> <li>k9s (a minimal TUI tool)</li> </ul> <p>k9s:</p> <p></p> <p>Rancher: </p>"},{"location":"kubernetes/#extra-custom-docker-registry","title":"Extra: Custom Docker Registry","text":"<p>Some big data services require custom Docker images. To simplify this, you can deploy a custom Docker registry within your Kubernetes cluster.</p> <p>Here\u2019s an example configuration for a Docker registry running on the <code>kube5</code> node:</p> registry.yaml <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: registry\n  namespace: default\n  labels:\n    app: registry\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: \"kube5\"\n  containers:\n  - name: registry\n    image: registry:latest\n    ports:\n      - containerPort: 5000\n    volumeMounts:\n    - name: registry-volume\n      mountPath: /var/lib/registry\n  volumes:\n  - name: registry-volume\n    hostPath:\n      path: /data/registry\n      type: Directory\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: registry-svc\n  namespace: default\nspec:\n  type: NodePort\n  selector:\n    app: registry\n  ports:\n  - port: 5000\n    targetPort: 5000\n    nodePort: 30123\n</code></pre> <p>Deploy the registry: <pre><code>kubectl apply -f registry.yaml\n</code></pre></p> <p>For insecure registries, update the Docker or containerd configuration on all nodes. For example, on Docker: <pre><code>{\n    \"insecure-registries\": [\"kube5:30123\"]\n}\n</code></pre></p> <p>Restart the Docker service: <pre><code>sudo systemctl restart docker\n</code></pre></p> <p>For containerd (used by RKE2), create <code>/etc/rancher/rke2/registries.yaml</code>: <pre><code>mirrors:\n  \"kube5:30123\":\n    endpoint:\n      - \"http://kube5:30123\"\n</code></pre></p> <p>Restart RKE2 services:</p> <ul> <li>Master Node: <pre><code>systemctl restart rke2-server\n</code></pre></li> <li>Worker Node: <pre><code>systemctl restart rke2-agent\n</code></pre></li> </ul> <p>Now, all Kubernetes nodes can access the custom Docker registry.</p>"},{"location":"ranger-hdfs-plugin/","title":"HDFS Plugin","text":""},{"location":"ranger-hdfs-plugin/#docker-image","title":"Docker Image","text":"<p>Copy the <code>ranger-3.0.0-SNAPSHOT-hdfs-plugin.tar.gz</code> file into the <code>hdfs/files</code> directory.</p> Dockerfile <p><pre><code>FROM apache/hadoop:3.4.1\n\nUSER root\nRUN mkdir /ranger\nRUN chown hadoop:hadoop /ranger\nRUN mkdir -p /etc/ranger\nRUN chown -R hadoop:root /etc/ranger\nRUN mkdir -p /var/log/hadoop/hdfs/audit/solr/spool\nRUN chown -R hadoop:root /var/log/hadoop/hdfs/audit/solr/spool\nUSER hadoop\nCOPY files/ranger-3.0.0-SNAPSHOT-hdfs-plugin.tar.gz /ranger/ranger-3.0.0-SNAPSHOT-hdfs-plugin.tar.gz\nWORKDIR /ranger\nRUN tar -xvf /ranger/ranger-3.0.0-SNAPSHOT-hdfs-plugin.tar.gz\nRUN chmod +x /ranger/ranger-3.0.0-SNAPSHOT-hdfs-plugin/enable-hdfs-plugin.sh\n\nWORKDIR /opt/hadoop\nUSER root\n</code></pre> This Dockerfile copies and extracts the plugin, then sets the necessary permissions.</p> <p>Build and push docker image: <pre><code>docker build -t kube5:30123/custom/hadoop:3.4.1 .\ndocker push kube5:30123/custom/hadoop:3.4.1 \n</code></pre></p>"},{"location":"ranger-hdfs-plugin/#plugin-configuration","title":"Plugin Configuration","text":"<p>Ranger HDFS plugin requires an <code>install.properties</code> file. A sample configuration is included in the plugin archive.</p> <p>Warning</p> <p>Update following properties:</p> <ul> <li><code>POLICY_MGR_URL=http://ranger.company.bigdata.svc.cluster.local:6080</code></li> <li><code>REPOSITORY_NAME=dev_hdfs</code></li> <li><code>COMPONENT_INSTALL_DIR_NAME=/opt/hadoop/</code></li> <li><code>XAAUDIT.SOLR.ENABLE=true</code></li> <li><code>XAAUDIT.SOLR.URL=http://192.168.1.65:30983/solr/ranger_audits</code></li> <li><code>XAAUDIT.HDFS.ENABLE=true</code></li> <li><code>XAAUDIT.HDFS.HDFS_DIR=hdfs://namenode.company.bigdata.svc.cluster.local:9000/ranger/audit</code></li> </ul> install.properties <pre><code># Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#\n# Location of Policy Manager URL  \n#\n# Example:\n# POLICY_MGR_URL=http://policymanager.xasecure.net:6080\n#\nPOLICY_MGR_URL=http://ranger.company.bigdata.svc.cluster.local:6080\n\n#\n# This is the repository name created within policy manager\n#\n# Example:\n# REPOSITORY_NAME=hadoopdev\n#\nREPOSITORY_NAME=dev_hdfs\n#\n# Set hadoop home when hadoop program and Ranger HDFS Plugin are not in the\n# same path.\n#\nCOMPONENT_INSTALL_DIR_NAME=/opt/hadoop/\n\n# AUDIT configuration with V3 properties\n# Enable audit logs to Solr\n#Example\n#XAAUDIT.SOLR.ENABLE=true\n#XAAUDIT.SOLR.URL=http://localhost:6083/solr/ranger_audits\n#XAAUDIT.SOLR.ZOOKEEPER=\n#XAAUDIT.SOLR.FILE_SPOOL_DIR=/var/log/hadoop/hdfs/audit/solr/spool\n\nXAAUDIT.SOLR.ENABLE=true\nXAAUDIT.SOLR.URL=http://192.168.1.65:30983/solr/ranger_audits\nXAAUDIT.SOLR.USER=NONE\nXAAUDIT.SOLR.PASSWORD=NONE\nXAAUDIT.SOLR.ZOOKEEPER=NONE\nXAAUDIT.SOLR.FILE_SPOOL_DIR=/var/log/hadoop/hdfs/audit/solr/spool\n\n# Enable audit logs to ElasticSearch\n#Example\n#XAAUDIT.ELASTICSEARCH.ENABLE=true\n#XAAUDIT.ELASTICSEARCH.URL=localhost\n#XAAUDIT.ELASTICSEARCH.INDEX=audit\n\nXAAUDIT.ELASTICSEARCH.ENABLE=false\nXAAUDIT.ELASTICSEARCH.URL=NONE\nXAAUDIT.ELASTICSEARCH.USER=NONE\nXAAUDIT.ELASTICSEARCH.PASSWORD=NONE\nXAAUDIT.ELASTICSEARCH.INDEX=NONE\nXAAUDIT.ELASTICSEARCH.PORT=NONE\nXAAUDIT.ELASTICSEARCH.PROTOCOL=NONE\n\n# Enable audit logs to HDFS\n#Example\n#XAAUDIT.HDFS.ENABLE=true\n#XAAUDIT.HDFS.HDFS_DIR=hdfs://node-1.example.com:8020/ranger/audit\n#XAAUDIT.HDFS.FILE_SPOOL_DIR=/var/log/hadoop/hdfs/audit/hdfs/spool\n#  If using Azure Blob Storage\n#XAAUDIT.HDFS.HDFS_DIR=wasb[s]://&lt;containername&gt;@&lt;accountname&gt;.blob.core.windows.net/&lt;path&gt;\n#XAAUDIT.HDFS.HDFS_DIR=wasb://ranger_audit_container@my-azure-account.blob.core.windows.net/ranger/audit\n\nXAAUDIT.HDFS.ENABLE=true\nXAAUDIT.HDFS.HDFS_DIR=hdfs://namenode.company.bigdata.svc.cluster.local:9000/ranger/audit\nXAAUDIT.HDFS.FILE_SPOOL_DIR=/var/log/hadoop/hdfs/audit/hdfs/spool\n\n# Following additional propertis are needed When auditing to Azure Blob Storage via HDFS\n# Get these values from your /etc/hadoop/conf/core-site.xml\n#XAAUDIT.HDFS.HDFS_DIR=wasb[s]://&lt;containername&gt;@&lt;accountname&gt;.blob.core.windows.net/&lt;path&gt;\nXAAUDIT.HDFS.AZURE_ACCOUNTNAME=__REPLACE_AZURE_ACCOUNT_NAME\nXAAUDIT.HDFS.AZURE_ACCOUNTKEY=__REPLACE_AZURE_ACCOUNT_KEY\nXAAUDIT.HDFS.AZURE_SHELL_KEY_PROVIDER=__REPLACE_AZURE_SHELL_KEY_PROVIDER\nXAAUDIT.HDFS.AZURE_ACCOUNTKEY_PROVIDER=__REPLACE_AZURE_ACCOUNT_KEY_PROVIDER\n\n#Log4j Audit Provider\nXAAUDIT.LOG4J.ENABLE=false\nXAAUDIT.LOG4J.IS_ASYNC=false\nXAAUDIT.LOG4J.ASYNC.MAX.QUEUE.SIZE=10240\nXAAUDIT.LOG4J.ASYNC.MAX.FLUSH.INTERVAL.MS=30000\nXAAUDIT.LOG4J.DESTINATION.LOG4J=true\nXAAUDIT.LOG4J.DESTINATION.LOG4J.LOGGER=xaaudit\n\n# Enable audit logs to Amazon CloudWatch Logs\n#Example\n#XAAUDIT.AMAZON_CLOUDWATCH.ENABLE=true\n#XAAUDIT.AMAZON_CLOUDWATCH.LOG_GROUP=ranger_audits\n#XAAUDIT.AMAZON_CLOUDWATCH.LOG_STREAM={instance_id}\n#XAAUDIT.AMAZON_CLOUDWATCH.FILE_SPOOL_DIR=/var/log/hive/audit/amazon_cloudwatch/spool\n\nXAAUDIT.AMAZON_CLOUDWATCH.ENABLE=false\nXAAUDIT.AMAZON_CLOUDWATCH.LOG_GROUP=NONE\nXAAUDIT.AMAZON_CLOUDWATCH.LOG_STREAM_PREFIX=NONE\nXAAUDIT.AMAZON_CLOUDWATCH.FILE_SPOOL_DIR=NONE\nXAAUDIT.AMAZON_CLOUDWATCH.REGION=NONE\n\n# End of V3 properties\n\n#\n#  Audit to HDFS Configuration\n#\n# If XAAUDIT.HDFS.IS_ENABLED is set to true, please replace tokens\n# that start with __REPLACE__ with appropriate values\n#  XAAUDIT.HDFS.IS_ENABLED=true\n#  XAAUDIT.HDFS.DESTINATION_DIRECTORY=hdfs://__REPLACE__NAME_NODE_HOST:8020/ranger/audit/%app-type%/%time:yyyyMMdd%\n#  XAAUDIT.HDFS.LOCAL_BUFFER_DIRECTORY=__REPLACE__LOG_DIR/hadoop/%app-type%/audit\n#  XAAUDIT.HDFS.LOCAL_ARCHIVE_DIRECTORY=__REPLACE__LOG_DIR/hadoop/%app-type%/audit/archive\n#\n# Example:\n#  XAAUDIT.HDFS.IS_ENABLED=true\n#  XAAUDIT.HDFS.DESTINATION_DIRECTORY=hdfs://namenode.example.com:8020/ranger/audit/%app-type%/%time:yyyyMMdd%\n#  XAAUDIT.HDFS.LOCAL_BUFFER_DIRECTORY=/var/log/hadoop/%app-type%/audit\n#  XAAUDIT.HDFS.LOCAL_ARCHIVE_DIRECTORY=/var/log/hadoop/%app-type%/audit/archive\n#\nXAAUDIT.HDFS.IS_ENABLED=false\nXAAUDIT.HDFS.DESTINATION_DIRECTORY=hdfs://__REPLACE__NAME_NODE_HOST:8020/ranger/audit/%app-type%/%time:yyyyMMdd%\nXAAUDIT.HDFS.LOCAL_BUFFER_DIRECTORY=__REPLACE__LOG_DIR/hadoop/%app-type%/audit\nXAAUDIT.HDFS.LOCAL_ARCHIVE_DIRECTORY=__REPLACE__LOG_DIR/hadoop/%app-type%/audit/archive\n\nXAAUDIT.HDFS.DESTINTATION_FILE=%hostname%-audit.log\nXAAUDIT.HDFS.DESTINTATION_FLUSH_INTERVAL_SECONDS=900\nXAAUDIT.HDFS.DESTINTATION_ROLLOVER_INTERVAL_SECONDS=86400\nXAAUDIT.HDFS.DESTINTATION_OPEN_RETRY_INTERVAL_SECONDS=60\nXAAUDIT.HDFS.LOCAL_BUFFER_FILE=%time:yyyyMMdd-HHmm.ss%.log\nXAAUDIT.HDFS.LOCAL_BUFFER_FLUSH_INTERVAL_SECONDS=60\nXAAUDIT.HDFS.LOCAL_BUFFER_ROLLOVER_INTERVAL_SECONDS=600\nXAAUDIT.HDFS.LOCAL_ARCHIVE_MAX_FILE_COUNT=10\n\n#Solr Audit Provider\nXAAUDIT.SOLR.IS_ENABLED=false\nXAAUDIT.SOLR.MAX_QUEUE_SIZE=1\nXAAUDIT.SOLR.MAX_FLUSH_INTERVAL_MS=1000\nXAAUDIT.SOLR.SOLR_URL=http://192.168.1.65:30983/solr/ranger_audits\n\n# End of V2 properties\n\n#\n# SSL Client Certificate Information\n#\n# Example:\n# SSL_KEYSTORE_FILE_PATH=/etc/hadoop/conf/ranger-plugin-keystore.jks\n# SSL_KEYSTORE_PASSWORD=none\n# SSL_TRUSTSTORE_FILE_PATH=/etc/hadoop/conf/ranger-plugin-truststore.jks\n# SSL_TRUSTSTORE_PASSWORD=none\n#\n# You do not need use SSL between agent and security admin tool, please leave these sample value as it is.\n#\nSSL_KEYSTORE_FILE_PATH=/etc/hadoop/conf/ranger-plugin-keystore.jks\nSSL_KEYSTORE_PASSWORD=myKeyFilePassword\nSSL_TRUSTSTORE_FILE_PATH=/etc/hadoop/conf/ranger-plugin-truststore.jks\nSSL_TRUSTSTORE_PASSWORD=changeit\n\n#\n# Custom component user\n# CUSTOM_COMPONENT_USER=&lt;custom-user&gt;\n# keep blank if component user is default\nCUSTOM_USER=hdfs\n\n\n#\n# Custom component group\n# CUSTOM_COMPONENT_GROUP=&lt;custom-group&gt;\n# keep blank if component group is default\nCUSTOM_GROUP=hadoop\n</code></pre> <p>Tip</p> <p>Ensure <code>XAAUDIT.SOLR.URL</code> is accessible from outside Kubernetes for audit logs.</p> <p>Create a ConfigMap for the plugin configuration: <pre><code>kubectl create configmap hdfs-ranger-config -n bigdata --from-file=install.properties=./files/install.properties\n</code></pre></p>"},{"location":"ranger-hdfs-plugin/#updating-the-ranger-service","title":"Updating the Ranger Service","text":"<p>In the Ranger UI, update the <code>dev_hdfs</code> service settings. </p> <p>Warning</p> <p>Ensure the following configurations are set:</p> <ul> <li><code>tag.download.auth.users</code></li> <li><code>policy.download.auth.users</code></li> <li><code>default.policy.users</code></li> </ul> <p>Without these, the plugin will fail to download policies from the Ranger Admin.</p> <p></p>"},{"location":"ranger-hdfs-plugin/#disable-hadoop-acls-optional","title":"Disable Hadoop ACLS (Optional)","text":"<p>By default, Ranger falls back to Hadoop ACLs if no policy matches. To disable this behavior, update the <code>ranger-hdfs-security.xml</code> file:</p> <p><pre><code>...\n  &lt;property&gt;\n        &lt;name&gt;xasecure.add-hadoop-authorization&lt;/name&gt;\n        &lt;value&gt;false&lt;/value&gt;\n        &lt;description&gt;\n            Enable/Disable the default hadoop authorization (based on\n            rwxrwxrwx permission on the resource) if Ranger Authorization fails.\n        &lt;/description&gt;\n    &lt;/property&gt;\n...\n</code></pre> Redeploy the ConfigMap: <pre><code>kubectl delete configmap hdfs-ranger-config -n bigdata \nkubectl create configmap hdfs-ranger-config -n bigdata --from-file=install.properties=./files/install.properties --from-file=ranger-hdfs-security.xml=./files/ranger-hdfs-security.xml\n</code></pre></p>"},{"location":"ranger-hdfs-plugin/#hdfs-configuration","title":"HDFS Configuration","text":"<p>Modify the <code>namenode.yaml</code> manifest to run the <code>enable-hdfs-plugin.sh</code> script as root. Since the Docker image runs as root, use the <code>runuser</code> command to start the NameNode as the <code>hadoop</code> user. Also, mount the <code>hdfs-ranger-config</code> ConfigMap.</p> <p><pre><code>...\n    image: kube5:30123/custom/hadoop:3.4.1\n    command: [\"/bin/bash\", \"-c\"]\n    args:\n    - | \n      /ranger/ranger-3.0.0-SNAPSHOT-hdfs-plugin/enable-hdfs-plugin.sh\n      if [ ! -d \"/hadoop/nn/current\" ]; then\n          runuser -u hadoop -- hdfs namenode -format -force\n      fi\n      runuser -u hadoop -- hdfs namenode\n\n    resources:\n...\n\nvolumeMounts:\n...\n    - name: hdfs-ranger-config\n      mountPath: /ranger/ranger-3.0.0-SNAPSHOT-hdfs-plugin/install.properties\n      subPath: install.properties\n    - name: hdfs-ranger-config\n      mountPath: /ranger/ranger-3.0.0-SNAPSHOT-hdfs-plugin/install/conf.templates/enable/ranger-hdfs-security.xml\n      subPath: ranger-hdfs-security.xml\n  volumes:\n...\n  - name: hdfs-ranger-config\n    configMap:\n      name: hdfs-ranger-config\n...\n</code></pre> Add the following property to <code>hdfs-site.xml</code> to enable the Ranger HDFS Authorizer: <pre><code>...\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.inode.attributes.provider.class&lt;/name&gt;\n        &lt;value&gt;org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer&lt;/value&gt;\n    &lt;/property&gt;\n...\n</code></pre></p>"},{"location":"ranger-hdfs-plugin/#verifying-the-plugin","title":"Verifying the Plugin","text":"<p>Once the plugin is successfully started, its status will appear in the Ranger UI under the Plugin Status page:</p> <p></p> <p>Audit logs will also be visible in the Ranger Audit section and also HDFS:</p> <p></p> <p></p>"},{"location":"ranger-hdfs-plugin/#example-hdfs-policies","title":"Example HDFS Policies","text":"<p>Here are some sample policies you can configure in Ranger:</p> <ul> <li> <p><code>/tmp</code> Path Permissions </p> </li> <li> <p><code>/user</code> Path Permissions </p> </li> <li> <p><code>hive</code> User Access to <code>warehouse</code> </p> </li> <li> <p><code>rangerlookup</code> Permissions </p> </li> </ul>"},{"location":"ranger-hive-plugin/","title":"Hive Plugin","text":""},{"location":"ranger-hive-plugin/#docker-image","title":"Docker Image","text":"<p>Copy the <code>ranger-3.0.0-SNAPSHOT-hive-plugin.tar.gz</code> file into the <code>hive/files</code> directory.</p> Dockerfile <p><pre><code>FROM apache/hive:4.0.1\n\nCOPY files/postgres.jar /opt/hive/lib/postgres.jar\n# Mysql: COPY files/mysql.jar /opt/hive/lib/mysql.jar\n\nUSER root\n\nRUN apt-get update &amp;&amp; \\\nDEBIAN_FRONTEND=noninteractive apt-get -qq -y install krb5-config krb5-user\n\nRUN mkdir -p /home/hive\nRUN chown -R hive:hive /home/hive\n\nRUN mkdir /ranger\nRUN chown hive:hive /ranger\nUSER hive\nCOPY files/ranger-3.0.0-SNAPSHOT-hive-plugin.tar.gz /ranger/ranger-3.0.0-SNAPSHOT-hive-plugin.tar.gz\nWORKDIR /ranger\n\nRUN tar -xvf /ranger/ranger-3.0.0-SNAPSHOT-hive-plugin.tar.gz\nRUN chmod +x /ranger/ranger-3.0.0-SNAPSHOT-hive-plugin/enable-hive-plugin.sh\n\nRUN mkdir -p /var/log/hive/audit/solr/spool\nRUN chown -R hive:root /var/log/hive/audit/solr/spool\nWORKDIR /opt/hive\n\nUSER root\n</code></pre> This Dockerfile copies and extracts the plugin, then sets the necessary permissions.</p> <p>Build and push docker image: <pre><code>docker build -t kube5:30123/custom/hive:4.0.1 .\ndocker push kube5:30123/custom/hive:4.0.1\n</code></pre></p>"},{"location":"ranger-hive-plugin/#plugin-configuration","title":"Plugin Configuration","text":"<p>Ranger Hive plugin requires an <code>install.properties</code> file. A sample configuration is included in the plugin archive.</p> <p>Warning</p> <p>Update following properties:</p> <ul> <li><code>POLICY_MGR_URL=http://ranger.company.bigdata.svc.cluster.local:6080</code></li> <li><code>REPOSITORY_NAME=dev_hive</code></li> <li><code>COMPONENT_INSTALL_DIR_NAME=/opt/hive</code></li> <li><code>XAAUDIT.SOLR.ENABLE=true</code></li> <li><code>XAAUDIT.SOLR.URL=http://192.168.1.65:30983/solr/ranger_audits</code></li> <li><code>XAAUDIT.HDFS.ENABLE=true</code></li> <li><code>XAAUDIT.HDFS.HDFS_DIR=hdfs://namenode.company.bigdata.svc.cluster.local:9000/ranger/audit</code></li> </ul> install.properties <pre><code># Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#\n# Location of Policy Manager URL  \n#\n# Example:\n# POLICY_MGR_URL=http://policymanager.xasecure.net:6080\n#\nPOLICY_MGR_URL=http://ranger.company.bigdata.svc.cluster.local:6080\n\n#\n# This is the repository name created within policy manager\n#\n# Example:\n# REPOSITORY_NAME=hivedev\n#\nREPOSITORY_NAME=dev_hive\n\n#\n# Hive installation directory\n#\n# Example:\n# COMPONENT_INSTALL_DIR_NAME=/var/local/apache-hive-2.1.0-bin\n#\nCOMPONENT_INSTALL_DIR_NAME=/opt/hive\n\n# AUDIT configuration with V3 properties\n\n# Enable audit logs to Solr\n#Example\n#XAAUDIT.SOLR.ENABLE=true\n#XAAUDIT.SOLR.URL=http://localhost:6083/solr/ranger_audits\n#XAAUDIT.SOLR.ZOOKEEPER=\n#XAAUDIT.SOLR.FILE_SPOOL_DIR=/var/log/hive/audit/solr/spool\n\nXAAUDIT.SOLR.ENABLE=true\nXAAUDIT.SOLR.URL=http://192.168.1.65:30983/solr/ranger_audits\nXAAUDIT.SOLR.USER=NONE\nXAAUDIT.SOLR.PASSWORD=NONE\nXAAUDIT.SOLR.ZOOKEEPER=NONE\nXAAUDIT.SOLR.FILE_SPOOL_DIR=/var/log/hive/audit/solr/spool\n\n# Enable audit logs to ElasticSearch\n#Example\n#XAAUDIT.ELASTICSEARCH.ENABLE=true\n#XAAUDIT.ELASTICSEARCH.URL=localhost\n#XAAUDIT.ELASTICSEARCH.INDEX=audit\n\nXAAUDIT.ELASTICSEARCH.ENABLE=false\nXAAUDIT.ELASTICSEARCH.URL=NONE\nXAAUDIT.ELASTICSEARCH.USER=NONE\nXAAUDIT.ELASTICSEARCH.PASSWORD=NONE\nXAAUDIT.ELASTICSEARCH.INDEX=NONE\nXAAUDIT.ELASTICSEARCH.PORT=NONE\nXAAUDIT.ELASTICSEARCH.PROTOCOL=NONE\n\n# Enable audit logs to HDFS\n#Example\n#XAAUDIT.HDFS.ENABLE=true\n#XAAUDIT.HDFS.HDFS_DIR=hdfs://node-1.example.com:8020/ranger/audit\n#  If using Azure Blob Storage\n#XAAUDIT.HDFS.HDFS_DIR=wasb[s]://&lt;containername&gt;@&lt;accountname&gt;.blob.core.windows.net/&lt;path&gt;\n#XAAUDIT.HDFS.HDFS_DIR=wasb://ranger_audit_container@my-azure-account.blob.core.windows.net/ranger/audit\n#XAAUDIT.HDFS.FILE_SPOOL_DIR=/var/log/hive/audit/hdfs/spool\n\nXAAUDIT.HDFS.ENABLE=true\nXAAUDIT.HDFS.HDFS_DIR=hdfs://namenode.company.bigdata.svc.cluster.local:9000/ranger/audit\nXAAUDIT.HDFS.FILE_SPOOL_DIR=/var/log/hive/audit/hdfs/spool\n\n# Following additional propertis are needed When auditing to Azure Blob Storage via HDFS\n# Get these values from your /etc/hadoop/conf/core-site.xml\n#XAAUDIT.HDFS.HDFS_DIR=wasb[s]://&lt;containername&gt;@&lt;accountname&gt;.blob.core.windows.net/&lt;path&gt;\nXAAUDIT.HDFS.AZURE_ACCOUNTNAME=__REPLACE_AZURE_ACCOUNT_NAME\nXAAUDIT.HDFS.AZURE_ACCOUNTKEY=__REPLACE_AZURE_ACCOUNT_KEY\nXAAUDIT.HDFS.AZURE_SHELL_KEY_PROVIDER=__REPLACE_AZURE_SHELL_KEY_PROVIDER\nXAAUDIT.HDFS.AZURE_ACCOUNTKEY_PROVIDER=__REPLACE_AZURE_ACCOUNT_KEY_PROVIDER\n\n#Log4j Audit Provider\nXAAUDIT.LOG4J.ENABLE=false\nXAAUDIT.LOG4J.IS_ASYNC=false\nXAAUDIT.LOG4J.ASYNC.MAX.QUEUE.SIZE=10240\nXAAUDIT.LOG4J.ASYNC.MAX.FLUSH.INTERVAL.MS=30000\nXAAUDIT.LOG4J.DESTINATION.LOG4J=true\nXAAUDIT.LOG4J.DESTINATION.LOG4J.LOGGER=xaaudit\n\n# Enable audit logs to Amazon CloudWatch Logs\n#Example\n#XAAUDIT.AMAZON_CLOUDWATCH.ENABLE=true\n#XAAUDIT.AMAZON_CLOUDWATCH.LOG_GROUP=ranger_audits\n#XAAUDIT.AMAZON_CLOUDWATCH.LOG_STREAM={instance_id}\n#XAAUDIT.AMAZON_CLOUDWATCH.FILE_SPOOL_DIR=/var/log/hive/audit/amazon_cloudwatch/spool\n\nXAAUDIT.AMAZON_CLOUDWATCH.ENABLE=false\nXAAUDIT.AMAZON_CLOUDWATCH.LOG_GROUP=NONE\nXAAUDIT.AMAZON_CLOUDWATCH.LOG_STREAM_PREFIX=NONE\nXAAUDIT.AMAZON_CLOUDWATCH.FILE_SPOOL_DIR=NONE\nXAAUDIT.AMAZON_CLOUDWATCH.REGION=NONE\n\n# End of V3 properties\n\n\n#\n#  Audit to HDFS Configuration\n#\n# If XAAUDIT.HDFS.IS_ENABLED is set to true, please replace tokens\n# that start with __REPLACE__ with appropriate values\n#  XAAUDIT.HDFS.IS_ENABLED=true\n#  XAAUDIT.HDFS.DESTINATION_DIRECTORY=hdfs://__REPLACE__NAME_NODE_HOST:8020/ranger/audit/%app-type%/%time:yyyyMMdd%\n#  XAAUDIT.HDFS.LOCAL_BUFFER_DIRECTORY=__REPLACE__LOG_DIR/hive/audit/%app-type%\n#  XAAUDIT.HDFS.LOCAL_ARCHIVE_DIRECTORY=__REPLACE__LOG_DIR/hive/audit/archive/%app-type%\n#\n# Example:\n#  XAAUDIT.HDFS.IS_ENABLED=true\n#  XAAUDIT.HDFS.DESTINATION_DIRECTORY=hdfs://namenode.example.com:8020/ranger/audit/%app-type%/%time:yyyyMMdd%\n#  XAAUDIT.HDFS.LOCAL_BUFFER_DIRECTORY=/var/log/hive/audit/%app-type%\n#  XAAUDIT.HDFS.LOCAL_ARCHIVE_DIRECTORY=/var/log/hive/audit/archive/%app-type%\n#\nXAAUDIT.HDFS.IS_ENABLED=false\nXAAUDIT.HDFS.DESTINATION_DIRECTORY=hdfs://__REPLACE__NAME_NODE_HOST:8020/ranger/audit/%app-type%/%time:yyyyMMdd%\nXAAUDIT.HDFS.LOCAL_BUFFER_DIRECTORY=__REPLACE__LOG_DIR/hive/audit/%app-type%\nXAAUDIT.HDFS.LOCAL_ARCHIVE_DIRECTORY=__REPLACE__LOG_DIR/hive/audit/archive/%app-type%\n\nXAAUDIT.HDFS.DESTINTATION_FILE=%hostname%-audit.log\nXAAUDIT.HDFS.DESTINTATION_FLUSH_INTERVAL_SECONDS=900\nXAAUDIT.HDFS.DESTINTATION_ROLLOVER_INTERVAL_SECONDS=86400\nXAAUDIT.HDFS.DESTINTATION_OPEN_RETRY_INTERVAL_SECONDS=60\nXAAUDIT.HDFS.LOCAL_BUFFER_FILE=%time:yyyyMMdd-HHmm.ss%.log\nXAAUDIT.HDFS.LOCAL_BUFFER_FLUSH_INTERVAL_SECONDS=60\nXAAUDIT.HDFS.LOCAL_BUFFER_ROLLOVER_INTERVAL_SECONDS=600\nXAAUDIT.HDFS.LOCAL_ARCHIVE_MAX_FILE_COUNT=10\n\n#Solr Audit Provider\nXAAUDIT.SOLR.IS_ENABLED=false\nXAAUDIT.SOLR.MAX_QUEUE_SIZE=1\nXAAUDIT.SOLR.MAX_FLUSH_INTERVAL_MS=1000\nXAAUDIT.SOLR.SOLR_URL=http://192.168.1.65:30983/solr/ranger_audits\n\n# End of V2 properties\n\n#\n# SSL Client Certificate Information\n#\n# Example:\n# SSL_KEYSTORE_FILE_PATH=/etc/hive/conf/ranger-plugin-keystore.jks\n# SSL_KEYSTORE_PASSWORD=none\n# SSL_TRUSTSTORE_FILE_PATH=/etc/hive/conf/ranger-plugin-truststore.jks\n# SSL_TRUSTSTORE_PASSWORD=none\n#\n# You do not need use SSL between agent and security admin tool, please leave these sample value as it is.\n#\nSSL_KEYSTORE_FILE_PATH=/etc/hive/conf/ranger-plugin-keystore.jks\nSSL_KEYSTORE_PASSWORD=myKeyFilePassword\nSSL_TRUSTSTORE_FILE_PATH=/etc/hive/conf/ranger-plugin-truststore.jks\nSSL_TRUSTSTORE_PASSWORD=changeit\n\n#\n# Should Hive GRANT/REVOKE update XA policies?\n#\n# Example:\n#     UPDATE_XAPOLICIES_ON_GRANT_REVOKE=true\n#     UPDATE_XAPOLICIES_ON_GRANT_REVOKE=false\n#\nUPDATE_XAPOLICIES_ON_GRANT_REVOKE=true\n\n#\n# Custom component user\n# CUSTOM_COMPONENT_USER=&lt;custom-user&gt;\n# keep blank if component user is default\nCUSTOM_USER=hive\n\n\n#\n# Custom component group\n# CUSTOM_COMPONENT_GROUP=&lt;custom-group&gt;\n# keep blank if component group is default\nCUSTOM_GROUP=hadoop\n</code></pre> <p>Tip</p> <p>Ensure <code>XAAUDIT.SOLR.URL</code> is accessible from outside Kubernetes for audit logs.</p> <p>Create a ConfigMap for the plugin configuration: <pre><code>kubectl create configmap hive-ranger-config -n bigdata --from-file=install.properties=./configs/install.properties\n</code></pre></p>"},{"location":"ranger-hive-plugin/#updating-the-ranger-service","title":"Updating the Ranger Service","text":"<p>In the Ranger UI, update the <code>dev_hive</code> service (Hadoop Sql) settings. </p> <p>Warning</p> <p>Ensure the following configurations are set:</p> <ul> <li><code>tag.download.auth.users</code></li> <li><code>policy.download.auth.users</code></li> <li><code>default.policy.users</code></li> </ul> <p>Without these, the plugin will fail to download policies from the Ranger Admin.</p> <p></p>"},{"location":"ranger-hive-plugin/#hive-configuration","title":"Hive Configuration","text":"<p>Modify the <code>metastore.yaml</code> and <code>hiveserver2.yaml</code> manifest to run the <code>enable-hive-plugin.sh</code> script as root. Since the Docker image runs as root, use the <code>runuser</code> command to start the metasore and server as the <code>hive</code> user. Also, mount the <code>hive-ranger-config</code> ConfigMap.</p> metastore.yaml/hiverserver.yaml <pre><code>...\n    ports:\n      - containerPort: 9083\n    command: [\"/bin/bash\", \"-c\"]\n    args:\n    - |\n      /ranger/ranger-3.0.0-SNAPSHOT-hive-plugin/enable-hive-plugin.sh\n      runuser -u hive -- /entrypoint.sh\n...\n\nvolumeMounts:\n...\n    - name: hive-ranger-config\n      mountPath: /ranger/ranger-3.0.0-SNAPSHOT-hive-plugin/install.properties\n      subPath: install.properties\n  volumes:\n...\n  - name: hive-ranger-config\n    configMap:\n      name: hive-ranger-config\n...\n</code></pre> <p>Add the following properties to <code>hive-site.xml</code> to enable the RangerHiveAuthorizer: <pre><code>...\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.pre.event.listeners&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.filter.hook&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.security.authorization.manager&lt;/name&gt;\n    &lt;value&gt;org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizerFactory&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.security.authenticator.manager&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator&lt;/value&gt;\n  &lt;/property&gt;\n...\n</code></pre></p>"},{"location":"ranger-hive-plugin/#verifying-the-plugin","title":"Verifying the Plugin","text":"<p>Once the plugin is successfully started, its status will appear in the Ranger UI under the Plugin Status page:</p> <p></p> <p>Audit logs will also be visible in the Ranger Audit section and also HDFS:</p> <p></p> <p></p>"},{"location":"ranger/","title":"Apache Ranger","text":"<p>Apache Ranger\u2122 is a framework designed to enable, monitor, and manage comprehensive data security across the Hadoop ecosystem.</p> <p>Ranger Admin provides a user interface for managing policies and auditing. Plugins periodically fetch policies from the Ranger Admin and must be installed in the same environment as the services they protect. In Kubernetes, these plugins need to run within the same container as the service. Therefore, we must create Docker images that include the Ranger plugins for Hive and HDFS.</p> <p>The Ranger installation process can be divided into two phases:</p> <ol> <li>Ranger Admin setup</li> <li>Service plugin installation</li> </ol>"},{"location":"ranger/#building-ranger","title":"Building Ranger","text":"<p>Apache Ranger does not provide pre-built Docker images or plugins. As a result, we need to build the Docker images and plugins manually before proceeding with the installation.</p> <p>Run the following commands to clone the Apache Ranger repository: <pre><code>git clone https://github.com/apache/ranger.git\ncd ranger\n</code></pre></p> <p>Warning</p> <p>The changes below are just workaround. link</p> <p>By default, Hive Metastore plugin support is disabled in the Ranger Hive Plugin. To enable it, modify the following section in <code>hive-agent/src/main/java/org/apache/ranger/authorization/hive/authorizer/RangerHiveAuthorizer.java</code>:</p> <p><pre><code>...\nif (sessionContext != null) {\n     switch (sessionContext.getClientType()) {\n          case HIVECLI:\n               appType = \"hiveCLI\";\n               break;\n\n          case HIVESERVER2:\n               appType = \"hiveServer2\";\n               break;\n\n          /* !!! UNCOMMENT !!! */\n          case HIVEMETASTORE:\n               appType = \"hiveMetastore\";\n               break;\n\n          case OTHER:\n               appType = \"other\";\n               break;\n     }\n}\n...\n</code></pre> To resolve a bug related to setting the owner of databases or tables, update the <code>setOwnerUser</code> function as follows: <pre><code>    static void setOwnerUser(RangerHiveResource resource, HivePrivilegeObject hiveObj, IMetaStoreClient metaStoreClient, Map&lt;String, String&gt; objOwners) {\n        if (hiveObj != null) {\n            resource.setOwnerUser(hiveObj.getOwnerName());\n        }\n\n        LOG.debug(\"setOwnerUser({}): ownerName={}\", hiveObj, resource.getOwnerUser());\n    }\n</code></pre></p> <p>After applying the necessary changes, build Ranger Docker images and plugins using the following command. Ensure Docker is installed and running on your system:</p> <pre><code>export ENABLED_RANGER_SERVICES=\"tagsync,hadoop,hbase,kafka,hive,knox,kms\"\n./ranger_in_docker up\n</code></pre> <p>Warning</p> <p>This process can take up to an hour to complete. </p> <p>Once the build is finished, the plugins will be available in the <code>ranger/dev-support/ranger-docker/dist/</code> directory.</p> <p></p> <p>The required Docker images will also be generated:</p> <p></p> <p>The build process creates a development environment with many unnecessary Docker services. You can disable them using the following command: <pre><code>./ranger_in_docker down\n</code></pre> For Hive and HDFS, we will need the following:</p> <ul> <li><code>ranger-3.0.0-SNAPSHOT-hive-plugin.tar.gz</code></li> <li><code>ranger-3.0.0-SNAPSHOT-hdfs-plugin.tar.gz</code></li> <li><code>ranger</code> and <code>ranger-usersync</code> Docker images</li> </ul> <p>Warning</p> <p>If you plan to use Ranger with other services, you will need the corresponding service plugins.</p> <p>Tag and push the <code>ranger</code> Docker image to your Docker repository: <pre><code>docker tag ranger:latest kube5:30123/ranger:latest\ndocker push kube5:30123/ranger:latest\n</code></pre></p>"},{"location":"ranger/#kerberos-configuration","title":"Kerberos Configuration","text":"<p>The Ranger Admin server requires the following Kerberos principals: <pre><code>HTTP/ranger.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nrangeradmin/ranger.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nrangerlookup/ranger.company.bigdata.svc.cluster.local@HOMELDAP.ORG\n</code></pre></p> <p>Deploy the Kerberos keytabs as a Kubernetes Secret: <pre><code>kubectl create -n bigdata secret generic keytab-ranger --from-file=spnego.keytab=./files/spnego.keytab --from-file=rangeradmin.keytab=./files/rangeradmin.keytab --from-file=rangerlookup.keytab=./files/rangerlookup.keytab\n</code></pre></p>"},{"location":"ranger/#solr-integration","title":"Solr Integration","text":"<p>Ranger uses Solr to monitor audit logs in real time. The required Solr configuration files are included in the repository. Deploy Solr and its configurations using the following commands:</p> <p><pre><code>kubectl create configmap solr-config -n bigdata --from-file=solrconfig.xml=./ranger_configs/solrconfig.xml --from-file=managed-schema=./ranger_configs/managed-schema\nkubectl apply -f solr.yaml\n</code></pre> You can access the Solr Admin UI via the NodePort service at http://dns_or_ip_of_any_k8s_node:30983.</p> <p></p>"},{"location":"ranger/#ranger-admin-setup","title":"Ranger Admin Setup","text":"<p>Ranger Admin requires configuration files. Below is an example of the minimal configuration needed. Update the database settings to match your environment.</p> <p>Ranger requires backend database, you may need to change related database configurations in accordance with your environment.</p> files/ranger-admin-install.propertiesfiles/log4j.xml <pre><code>PYTHON_COMMAND_INVOKER=python3\nRANGER_ADMIN_LOG_DIR=/var/log/ranger\nRANGER_PID_DIR_PATH=/var/run/ranger\nDB_FLAVOR=POSTGRES\nSQL_CONNECTOR_JAR=/usr/share/java/postgresql.jar\nRANGER_ADMIN_LOGBACK_CONF_FILE=/opt/ranger/admin/ews/webapp/WEB-INF/classes/conf/logback.xml\n\ndb_root_user=root\ndb_root_password=142536\ndb_host=192.168.122.18:5432\n\ndb_name=ranger\ndb_user=root\ndb_password=142536\n\npostgres_core_file=db/postgres/optimized/current/ranger_core_db_postgres.sql\npostgres_audit_file=db/postgres/xa_audit_db_postgres.sql\nmysql_core_file=db/mysql/optimized/current/ranger_core_db_mysql.sql\nmysql_audit_file=db/mysql/xa_audit_db.sql\n\nrangerAdmin_password=rangerR0cks!\nrangerTagsync_password=rangerR0cks!\nrangerUsersync_password=rangerR0cks!\nkeyadmin_password=rangerR0cks!\n\n\naudit_store=solr\naudit_solr_urls=http://solr.company.bigdata.svc.cluster.local:8983/solr/ranger_audits\naudit_solr_collection_name=ranger_audits\n\n# audit_store=elasticsearch\naudit_elasticsearch_urls=\naudit_elasticsearch_port=9200\naudit_elasticsearch_protocol=http\naudit_elasticsearch_user=elastic\naudit_elasticsearch_password=elasticsearch\naudit_elasticsearch_index=ranger_audits\naudit_elasticsearch_bootstrap_enabled=true\n\npolicymgr_external_url=http://ranger.company.bigdata.svc.cluster.local:6080\npolicymgr_http_enabled=true\n\nunix_user=ranger\nunix_user_pwd=ranger\nunix_group=ranger\n\n# Following variables are referenced in db_setup.py. Do not remove these\noracle_core_file=\nsqlserver_core_file=\nsqlanywhere_core_file=\ncred_keystore_filename=\n\n\n# #------------ Kerberos Config -----------------\nspnego_principal= HTTP/ranger.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nspnego_keytab= /etc/security/keytabs/spnego.keytab\ntoken_valid=30\ncookie_domain=\ncookie_path=/\nadmin_principal= rangeradmin/ranger.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nadmin_keytab= /etc/security/keytabs/rangeradmin.keytab\nlookup_principal= rangerlookup/ranger.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nlookup_keytab= /etc/security/keytabs/rangerlookup.keytab\nhadoop_conf=/hadoop-conf\n\n# #################  DO NOT MODIFY ANY VARIABLES BELOW #########################\n#\n# --- These deployment variables are not to be modified unless you understand the full impact of the changes\n#\n################################################################################\nXAPOLICYMGR_DIR=$PWD\napp_home=$PWD/ews/webapp\nTMPFILE=$PWD/.fi_tmp\nLOGFILE=$PWD/logfile\nLOGFILES=\"$LOGFILE\"\n\nJAVA_BIN='java'\nJAVA_VERSION_REQUIRED='1.8'\n\nranger_admin_max_heap_size=1g\n#retry DB and Java patches after the given time in seconds.\nPATCH_RETRY_INTERVAL=120\nSTALE_PATCH_ENTRY_HOLD_TIME=10\n\nauthentication_method=NONE\n</code></pre> <pre><code>&lt;category name=\"org.apache.ranger\" additivity=\"false\"&gt;\n&lt;priority value=\"debug\" /&gt;\n&lt;appender-ref ref=\"xa_log_appender\" /&gt;\n&lt;/category&gt;\n</code></pre> <p>Deploy ranger configurations as ConfigMap: <pre><code>kubectl create configmap ranger-config -n bigdata --from-file=ranger-admin-install.properties=./configs/ranger-admin-install.properties --from-file=log4j.xml=./configs/log4j.xml\n</code></pre></p> <p>Use the following YAML file to deploy Ranger Admin:</p> ranger.yaml <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ranger\n  namespace: bigdata\n  labels:\n    name: ranger\n    app: ranger\n    dns: hdfs-subdomain\nspec:\n  hostname: ranger\n  subdomain: company\n  containers:\n  - name: ranger\n    image: kube5:30123/ranger:latest\n    imagePullPolicy: Always\n    resources:\n      limits:\n        memory: \"2G\"\n        cpu: \"500m\"\n    volumeMounts:\n    - name: ranger-config\n      mountPath: /opt/ranger/admin/install.properties\n      subPath: ranger-admin-install.properties\n    - name: ranger-config\n      mountPath: /opt/ranger/admin/ews/webapp/WEB-INF/log4j.xml\n      subPath: log4j.xml\n    - name: keytab-ranger\n      mountPath: /etc/security/keytabs/\n    - name: hadoop-config\n      mountPath: /hadoop-conf\n    - name: krb5conf\n      mountPath: /etc/krb5.conf\n      subPath: krb5.conf\n  volumes:\n  - name: ranger-config\n    configMap:\n      name: ranger-config\n  - name: hadoop-config\n    configMap:\n      name: hadoop-config\n  - name: keytab-ranger\n    secret:\n      secretName: keytab-ranger\n  - name: krb5conf\n    configMap:\n      name: krb5conf\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ranger-np\n  namespace: bigdata\nspec:\n  type: NodePort\n  selector:\n    app: ranger\n  ports:\n  - port: 6080\n    targetPort: 6080\n    nodePort: 30680\n</code></pre> <p><pre><code>kubectl apply -f ranger.yaml\n</code></pre> The firs run takes some minutes. Once deployed, you can access the Ranger Admin UI at http://dns_or_ip_of_any_k8s_node:30680</p> <p>Username: <code>admin</code> Password: <code>rangerR0cks!</code></p> <p></p>"},{"location":"ranger/#ranger-usersync","title":"Ranger UserSync","text":"<p>Ranger UserSync periodically synchronizes users from LDAP or UNIX into the Ranger database. If you don\u2019t need this feature, you can manually add users via the Ranger Admin UI under Admin &gt; Users. Ranger only verifies the username, so you can assign any password.</p> <p>The <code>ranger-usersync</code> Docker image built earlier can be used if you decide to enable UserSync in the future.</p>"},{"location":"secure-hdfs/","title":"Secure HDFS","text":""},{"location":"secure-hdfs/#creating-required-principals-and-certificates","title":"Creating required principals and certificates","text":""},{"location":"secure-hdfs/#kerberos-principals","title":"Kerberos Principals","text":"<p>HDFS requires both <code>hdfs</code> and <code>host</code> principals for each node. In this example, the Kerberos realm is HOMELDAP.ORG. The following principals need to be created: <pre><code>hdfs/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nhost/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nhdfs/datanode01.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nhost/datanode01.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nhdfs/datanode02.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nhost/datanode02.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nhdfs/datanode03.company.bigdata.svc.cluster.local@HOMELDAP.ORG\nhost/datanode03.company.bigdata.svc.cluster.local@HOMELDAP.ORG\n</code></pre></p> <p>Generate keytabs using <code>ktutil</code>:</p> <pre><code>ktutil\nadd_entry -password -p hdfs/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes128-cts-hmac-sha1-96\nadd_entry -password -p hdfs/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes256-cts-hmac-sha1-96\nadd_entry -password -p hdfs/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes128-cts-hmac-sha256-128\nadd_entry -password -p host/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes128-cts-hmac-sha1-96\nadd_entry -password -p host/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes256-cts-hmac-sha1-96\nadd_entry -password -p host/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes128-cts-hmac-sha256-128\nwkt ./files/namenode.keytab\n</code></pre> <p>These keytabs should be deployed as Kubernetes Secrets:</p> <pre><code>kubectl create -n bigdata secret generic keytab-hdfs-namenode --from-file=./files/namenode.keytab\nkubectl create -n bigdata secret generic keytab-hdfs-datanode-01 --from-file=datanode.keytab=./files/datanode01.keytab\nkubectl create -n bigdata secret generic keytab-hdfs-datanode-02 --from-file=datanode.keytab=./files/datanode02.keytab\nkubectl create -n bigdata secret generic keytab-hdfs-datanode-03 --from-file=datanode.keytab=./files/datanode03.keytab\n</code></pre>"},{"location":"secure-hdfs/#krb5conf","title":"krb5.conf","text":"<p>To allow Hadoop services to communicate with the KDC, a custom <code>krb5.conf</code> file is needed. Below is a sample configuration:</p> krb5.conf <p><pre><code>[libdefaults]\ndefault_realm = HOMELDAP.ORG\ndns_canonicalize_hostname = true\n[realms]\nHOMELDAP.ORG = {\n        admin_server = kdc.homeldap.org\n        kdc = kdc.homeldap.org\n}\n</code></pre> Deploy it using: <pre><code>kubectl create configmap krb5conf -n bigdata --from-file=krb5.conf=./files/krb5.conf\n</code></pre></p>"},{"location":"secure-hdfs/#ssl-certificates","title":"SSL Certificates","text":"<p>To enable secure communication, we need to generate a certificate, along with keystore and truststore files. It's important that the Common Name (CN) or Subject Alternative Names (SANs) in the certificate include the HDFS node hostnames. Wildcard certificates are supported by HDFS. <pre><code>DOMAIN=\"*.company.bigdata.svc.cluster.local\"    # Replace with your domain\n</code></pre> A script for generating the required self-signed certificate and associated files is available in the GitHub repository.</p> <p>Deploy the keystore and truststore as a Kubernetes Secret:</p> <pre><code>kubectl create -n bigdata secret generic keystore-hdfs --from-file=keystore=./files/keystore --from-file=truststore=./files/truststore\n</code></pre>"},{"location":"secure-hdfs/#configurations","title":"Configurations","text":"<p>Update the following configuration files: <code>core-site.xml</code> and <code>hdfs-site.xml</code>.</p> configs/core-site.xmlconfigs/hdfs-site.xml <pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n      &lt;name&gt;fs.defaultFS&lt;/name&gt;\n      &lt;value&gt;hdfs://namenode.company.bigdata.svc.cluster.local:9000&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;!-- Kerberos Configs --&gt;\n  &lt;property&gt;\n      &lt;name&gt;hadoop.security.authentication&lt;/name&gt;\n      &lt;value&gt;kerberos&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;hadoop.security.authorization&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;hadoop.rpc.protection&lt;/name&gt;\n      &lt;value&gt;authentication&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;\n      &lt;value&gt;\n        RULE:[2:$1@$0](hdfs@HOMELDAP.ORG)s/(.*)@HOMELDAP.ORG/hdfs/\n        RULE:[2:$1@$0](hive@HOMELDAP.ORG)s/(.*)@HOMELDAP.ORG/hive/\n        RULE:[2:$1@$0](rangerlookup@HOMELDAP.ORG)s/(.*)@HOMELDAP.ORG/rangerlookup/\n        RULE:[2:$1@$0](rangeradmin@HOMELDAP.ORG)s/(.*)@HOMELDAP.ORG/rangeradmin/\n        DEFAULT\n      &lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n      &lt;name&gt;hadoop.sql.require.client.cert&lt;/name&gt;\n      &lt;value&gt;false&lt;/value&gt;\n  &lt;/property&gt;\n\n\n&lt;/configuration&gt;\n</code></pre> <pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n      &lt;value&gt;/hadoop/nn&lt;/value&gt;\n      &lt;description&gt;Determines where on the local filesystem the DFS name node\n          should store the name table. If this is a comma-delimited list\n          of directories then the name table is replicated in all of the\n          directories, for redundancy. &lt;/description&gt;\n      &lt;final&gt;true&lt;/final&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n      &lt;value&gt;/hadoop/disk1&lt;/value&gt;\n      &lt;description&gt;Determines where on the local filesystem an DFS data node\n          should store its blocks. If this is a comma-delimited\n          list of directories, then data will be stored in all named\n          directories, typically on different devices.\n          Directories that do not exist are ignored.\n      &lt;/description&gt;\n      &lt;final&gt;true&lt;/final&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.webhdfs.enable&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt;\n      &lt;value&gt;3&lt;/value&gt;\n      &lt;description&gt;Determines datanode heartbeat interval in seconds.\n      &lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.http.address&lt;/name&gt;\n      &lt;value&gt;0.0.0.0:50070&lt;/value&gt;\n      &lt;description&gt;The name of the default file system. Either the\n          literal string \"local\" or a host:port for NDFS.\n      &lt;/description&gt;\n      &lt;final&gt;true&lt;/final&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.https.port&lt;/name&gt;\n      &lt;value&gt;50470&lt;/value&gt;\n      &lt;description&gt;The https port where namenode binds&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.https.address&lt;/name&gt;\n      &lt;value&gt;0.0.0.0:50470&lt;/value&gt;\n      &lt;description&gt;The https address where namenode binds&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.namenode.datanode.registration.ip-hostname-check&lt;/name&gt;\n      &lt;value&gt;false&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.permissions&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.permissions.ContentSummary.subAccess&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;\n      &lt;value&gt;hdfs/_HOST@HOMELDAP.ORG&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;\n      &lt;value&gt;/etc/security/keytab/namenode.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.namenode.kerberos.internal.spnego.principal&lt;/name&gt;\n      &lt;value&gt;hdfs/_HOST@HOMELDAP.ORG&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt;\n      &lt;value&gt;false&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;\n      &lt;value&gt;700&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.datanode.address&lt;/name&gt;\n      &lt;value&gt;0.0.0.0:9866&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.datanode.http.address&lt;/name&gt;\n      &lt;value&gt;0.0.0.0:9868&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.datanode.https.address&lt;/name&gt;\n      &lt;value&gt;0.0.0.0:9865&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt;\n      &lt;value&gt;hdfs/_HOST@HOMELDAP.ORG&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt;\n      &lt;value&gt;/etc/security/keytab/datanode.keytab&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt;\n      &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.encrypt.data.transfer.algorithm&lt;/name&gt;\n      &lt;value&gt;rc4&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;\n      &lt;value&gt;integrity&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n      &lt;name&gt;dfs.http.policy&lt;/name&gt;\n      &lt;value&gt;HTTPS_ONLY&lt;/value&gt;\n  &lt;/property&gt;\n\n&lt;/configuration&gt;\n</code></pre> <p>To enable SSL, create a separate configuration file named <code>ssl-server.xml</code>:</p> <p>Be sure to change the default passwords for both the keystore and truststore.</p> configs/ssl-server.xml <pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.keystore.location&lt;/name&gt;\n        &lt;value&gt;/etc/security/ssl/keystore&lt;/value&gt;\n        &lt;description&gt;Keystore to be used. Must be specified.\n        &lt;/description&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.keystore.password&lt;/name&gt;\n        &lt;value&gt;Bigdata1&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.keystore.keypassword&lt;/name&gt;\n        &lt;value&gt;Bigdata1&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.keystore.type&lt;/name&gt;\n        &lt;value&gt;JKS&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.truststore.type&lt;/name&gt;\n        &lt;value&gt;JKS&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.truststore.location&lt;/name&gt;\n        &lt;value&gt;/etc/security/ssl/truststore&lt;/value&gt;\n        &lt;description&gt;Truststore to be used by NN and DN. Must be specified.\n        &lt;/description&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;ssl.server.truststore.password&lt;/name&gt;\n        &lt;value&gt;Bigdata1&lt;/value&gt;\n        &lt;description&gt;Optional. Default value is \"\".\n        &lt;/description&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>To apply the updated configuration, delete and recreate the config map:</p> <pre><code>kubectl delete configmap hadoop-config -n bigdata\nkubectl create configmap hadoop-config -n bigdata --from-file=core-site.xml=./configs/core-site.xml --from-file=hdfs-site.xml=./configs/hdfs-site.xml  --from-file=ssl-server.xml=./configs/ssl-server.xml\n</code></pre>"},{"location":"secure-hdfs/#kdc-server-dns","title":"KDC Server DNS","text":"<p>To resolve the KDC server inside pods, use the <code>hostAliases</code> feature to update the <code>/etc/hosts</code> file. This step may not be required if your cluster is configured with DNS resolution for the KDC.</p> namenode.yaml/datanode_01.yaml <p><pre><code>...\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: \"kube1\"\n  hostname: namenode\n  subdomain: company\n  hostAliases:\n  - ip: \"192.168.1.52\"\n    hostnames:\n    - \"kdc.homeldap.org\"\n  containers:\n  - name: namenode\n    image: apache/hadoop:3.4.1\n...\n</code></pre> Make sure the IP address corresponds to the actual IP of KDC server.</p>"},{"location":"secure-hdfs/#mounting-configs-and-secrets","title":"Mounting Configs and Secrets","text":"<p>Mount the configuration files and secrets in Hadoop service manifests.</p> namenode.yamldatanode_01.yaml <pre><code>...\n\nvolumeMounts:\n- name: hadoop-config\n  mountPath: /opt/hadoop/etc/hadoop/ssl-server.xml\n  subPath: ssl-server.xml\n- name: krb5conf\n  mountPath: /etc/krb5.conf\n  subPath: krb5.conf\n- name: keytab-hdfs-namenode\n  mountPath: /etc/security/keytab/namenode.keytab\n  subPath: namenode.keytab\n- name: keystore-hdfs\n  mountPath: /etc/security/ssl/\n\n...\n\nvolumes:\n- name: krb5conf\n  configMap:\n    name: krb5conf\n- name: keytab-hdfs-namenode\n  secret:\n    secretName: keytab-hdfs-namenode\n- name: keystore-hdfs\n  secret:\n    secretName: keystore-hdfs\n...\n</code></pre> <pre><code>...\n\nvolumeMounts:\n- name: hadoop-config\n  mountPath: /opt/hadoop/etc/hadoop/ssl-server.xml\n  subPath: ssl-server.xml\n- name: krb5conf\n  mountPath: /etc/krb5.conf\n  subPath: krb5.conf\n- name: keytab-hdfs-datanode-01\n  mountPath: /etc/security/keytab/datanode.keytab\n  subPath: datanode.keytab\n- name: keystore-hdfs\n  mountPath: /etc/security/ssl/\n\n...\n\nvolumes:\n- name: krb5conf\n  configMap:\n    name: krb5conf\n- name: keytab-hdfs-datanode-01\n  secret:\n    secretName: keytab-hdfs-datanode-01\n- name: keystore-hdfs\n  secret:\n    secretName: keystore-hdfs\n...\n</code></pre> <p>Finally, redeploy the services: <pre><code>kubectl apply -f namenode.yaml\nkubectl apply -f datanode_01.yaml\nkubectl apply -f datanode_02.yaml\nkubectl apply -f datanode_03.yaml\n</code></pre> You can access secure namenode WEB UI via https://dns_or_ip_of_any_k8s_node:30570</p> <p></p>"},{"location":"secure-hdfs/#test","title":"Test","text":"<p>To test the setup, start by accessing the NameNode container: <pre><code>kubectl -n bigdata exec -it namenode -- bash\n</code></pre> Running HDFS commands without obtaining a Kerberos ticket will result in an authentication error:</p> <p></p> <p>To authenticate, run the following command:</p> <p><pre><code>kinit -kt /etc/security/keytab/namenode.keytab hdfs/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG\n</code></pre> Once authenticated, HDFS commands should work as expected:</p> <p></p>"},{"location":"todo/","title":"TODO","text":"<ul> <li>Spark-Jupyter Notebook-Jupyter Enterprise Gateway (JEG)</li> <li>Airflow - https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/kubernetes.html</li> <li>Nifi - https://konpyutaika.github.io/nifikop/</li> <li>Spark Operator - https://github.com/kubeflow/spark-operator</li> <li>Object Storage Integration - https://min.io/ https://ozone.apache.org/</li> <li>HDFS Cluster Kubernetes Operator (go)</li> <li>Kafka - https://github.com/strimzi/strimzi-kafka-operator</li> <li>Hbase/Cassandra</li> <li>Superset</li> <li>Open Policy Agent (OPA)</li> <li>Unity Catalog</li> <li>Keycloak</li> </ul>"},{"location":"trino/","title":"Trino","text":"<p>Trino,fast distributed SQL query engine for big data analytics that helps you explore your data universe.</p>"},{"location":"trino/#helm-chart","title":"Helm Chart","text":"<p>Pull and extract helm chart:</p> <pre><code>helm pull trino/trino\ntar -xvzf trino-1.35.0.tgz\nmv trino chart\n</code></pre>"},{"location":"trino/#kerberos-authentication","title":"Kerberos Authentication","text":"<p>Trino requires the following Kerberos principal:</p> <pre><code>trino@HOMELDAP.ORG # user to access other service for example HDFS, Hive etc.\nHTTP/trino-coordinator.example.com@HOMELDAP.ORG # user that using kerberos authentication\n</code></pre> <p><code>trino-coordinator.example.com</code> should be access URL of the coordinator. I used <code>kube1</code> as coordinator URL. My <code>trino.keytab</code> looks like below: <pre><code>&gt;klist -kte files/trino.keytab \nKeytab name: FILE:files/trino.keytab\nKVNO Timestamp           Principal\n---- ------------------- ------------------------------------------------------\n   1 01/14/2025 00:17:05 trino@HOMELDAP.ORG (aes128-cts-hmac-sha1-96) \n   1 01/14/2025 00:17:05 trino@HOMELDAP.ORG (aes256-cts-hmac-sha1-96) \n   1 01/14/2025 00:17:05 trino@HOMELDAP.ORG (aes128-cts-hmac-sha256-128) \n   1 04/21/2025 09:36:33 HTTP/kube1@HOMELDAP.ORG (aes128-cts-hmac-sha1-96) \n   1 04/21/2025 09:36:33 HTTP/kube1@HOMELDAP.ORG (aes256-cts-hmac-sha1-96) \n   1 04/21/2025 09:36:33 HTTP/kube1@HOMELDAP.ORG (aes128-cts-hmac-sha256-128) \n</code></pre></p> <p>Deploy the Kerberos keytab as a Kubernetes Secret: <pre><code>kubectl create -n bigdata secret generic keytab-trino --from-file=./files/trino.keytab\n</code></pre></p>"},{"location":"trino/#tlshttps-settings","title":"TLS/HTTPS Settings","text":"<p>When using Kerberos authentication, access to the Trino coordinator must be through TLS and HTTPS.</p> <p>Tip</p> <p>You can use <code>ssl.sh</code> to create self-signed certs, in the repository. </p> <p>Deploy keystore as Kubernetes secret: <pre><code>kubectl create -n bigdata secret generic trino-keystore --from-file=./files/keystore\n</code></pre></p>"},{"location":"trino/#hadoop-proxy-userimpersonation","title":"Hadoop Proxy User/Impersonation","text":"<p>Add trino user to <code>core-site.xml</code>:</p> core-site.xml <pre><code>...\n   &lt;property&gt;\n      &lt;name&gt;hadoop.proxyuser.trino.hosts&lt;/name&gt;\n      &lt;value&gt;*&lt;/value&gt;\n   &lt;/property&gt;\n   &lt;property&gt;\n      &lt;name&gt;hadoop.proxyuser.trino.groups&lt;/name&gt;\n      &lt;value&gt;*&lt;/value&gt;\n   &lt;/property&gt;\n   &lt;property&gt;\n      &lt;name&gt;hadoop.proxyuser.trino.users&lt;/name&gt;\n      &lt;value&gt;*&lt;/value&gt;\n   &lt;/property&gt;\n... \n</code></pre>"},{"location":"trino/#configuration","title":"Configuration","text":"<p>Tip</p> <p>We are using Hive metastore as iceberg catalog. You can add more connections to Trino. </p> <p>Tip</p> <p>Trino provides very detailed helm chart documentation. </p> values.yaml <pre><code>server:\n  workers: 1\n  coordinatorExtraConfig: |\n    internal-communication.shared-secret=aPM9WVbktUsslZ8amzrPmdCwQjUiMBcqTJMotIK7BrzLzANCYJQm6170qYMj//Oo+NKS7I9UrzdN\n    http-server.authentication.type=KERBEROS\n    http-server.authentication.krb5.service-name=HTTP\n    http-server.authentication.krb5.principal-hostname=kube1\n    http-server.authentication.krb5.keytab=/etc/security/keytabs/trino.keytab\n    http-server.authentication.krb5.user-mapping.pattern = (.*)(@.*)\n    http.authentication.krb5.config=/etc/krb5.conf\n    http-server.https.enabled=true\n    http-server.https.port=7778\n    http-server.https.keystore.path=/etc/security/certs/keystore\n    http-server.https.keystore.key=Bigdata1\n  workerExtraConfig: |\n    internal-communication.shared-secret=aPM9WVbktUsslZ8amzrPmdCwQjUiMBcqTJMotIK7BrzLzANCYJQm6170qYMj//Oo+NKS7I9UrzdN\n\nimage:\n  tag: \"474\"\ncoordinator:\n  configMounts:\n  - name: hadoop-config\n    configMap: hadoop-config\n    path: /hadoop-conf\n  - name: hive-site-config\n    configMap: hive-site-config\n    path: /hive-conf  \nworker:\n  configMounts:\n  - name: hadoop-config\n    configMap: hadoop-config\n    path: /hadoop-conf \n  - name: hive-site-config\n    configMap: hive-site-config\n    path: /hive-conf  \n\nconfigMounts:\n - name: krb5conf\n   configMap: krb5conf\n   path: /etc/krb5.conf\n   subPath: krb5.conf\n\nsecretMounts:\n- name: keytab-trino\n  secretName: keytab-trino\n  path: /etc/security/keytabs/\n- name: trino-keystore\n  secretName: trino-keystore\n  path: /etc/security/certs/  \n\nadditionalLogProperties:\n- io.trino.plugin.hive.metastore.thrif=DEBUG\ncatalogs:\n  lakehouse: |-\n    connector.name=iceberg\n    hive.metastore.uri=thrift://metastore.company.bigdata.svc.cluster.local:9083\n    fs.hadoop.enabled = true\n    hive.config.resources = /hadoop-conf/core-site.xml, /hadoop-conf/hdfs-site.xml, /hive-conf/hive-site.xml\n    hive.hdfs.impersonation.enabled=false\n    hive.hdfs.authentication.type = KERBEROS\n    hive.hdfs.trino.principal = trino@HOMELDAP.ORG\n    hive.hdfs.trino.keytab = /etc/security/keytabs/trino.keytab\n    hive.metastore.authentication.type=KERBEROS\n    hive.metastore.thrift.impersonation.enabled=true\n    hive.metastore.service.principal=hive/_HOST@HOMELDAP.ORG\n    hive.metastore.client.principal=trino@HOMELDAP.ORG\n    hive.metastore.client.keytab=/etc/security/keytabs/trino.keytab\n    hive.metastore.thrift.client.connect-timeout=1h\n    hive.metastore.thrift.client.read-timeout=1h\n    iceberg.metadata-cache.enabled=false\n\naccessControl:\n  type: configmap\n  configFile: \"rules.json\"\n  rules:\n    rules.json: |-\n      {\n        \"impersonation\": [\n          {\n            \"original_user\": \"trino\",\n            \"new_user\": \".*\",\n            \"allow\": true\n\n          },\n          {\n            \"original_user\": \"hue\",\n            \"new_user\": \".*\",\n            \"allow\": true\n\n          }\n        ]\n      }\n</code></pre> <p>Tip</p> <p>Modify <code>deployment-coordinator.yaml</code> and <code>deployment-worker.yaml</code> in Helm chart to mount <code>krb5.conf</code>correctly and to add Kerberos server DNS records: <pre><code>...\n spec:\n  hostAliases:\n  - ip: \"192.168.1.52\"\n    hostnames:\n    - \"kdc.homeldap.org\"\n  serviceAccountName: \n...\n {{- range .Values.configMounts }}\n  - name: {{ .name }}\n    mountPath: {{ .path }}\n    subPath: {{ .subPath }}\n {{- end }}\n...\n</code></pre></p> <p>Deploy Trino: <pre><code>helm install -f values.yaml trino ./chart -n bigdata\n</code></pre></p> <p></p> <p>If you have a valid Kerberos ticket, you should access to Trino using Kerberos authentication. You may need do some configuration on your browser settings. Follow Cern Kerberos guide for required settings.</p>"},{"location":"trino/#ranger-policies","title":"Ranger policies","text":"<p>Add trino user to <code>/warehouse</code> path access policy:</p> <p></p>"},{"location":"trino/#access-trino-using-hue","title":"Access Trino using Hue","text":"<p>Add trino configurations to Hue <code>values.yaml</code> as interpreter. <pre><code>...\n     name: \"hue\" # You must create hue database before deployment\n  interpreters: |\n    [[[trino]]]\n      name = Trino\n      interface=sqlalchemy\n      options='{\"url\": \"trino://kube1:30808/lakehouse\", \"has_impersonation\": true, \"connect_args\": \"{\\\"protocol\\\": \\\"https\\\", \\\"KerberosKeytabPath\\\":\\\"/etc/security/keytabs/hue.keytab\\\", \\\"KerberosPrincipal\\\":\\\"hue@HOMELDAP.ORG\\\", \\\"KerberosRemoteServiceName\\\":\\\"HTTP\\\",\\\"KerberosCredentialCachePath\\\":\\\"/tmp/krb5cc_1001\\\",\\\"requests_kwargs\\\": {\\\"verify\\\": false}}\"}'\n  ini: |\n    [desktop]\n...\n</code></pre> Upgrade Hue helm: <pre><code>helm upgrade hue ./chart -n bigdata -f values.yaml # Update\n</code></pre></p> <p>Now you should access Trino using Hue.</p> <p></p>"},{"location":"trino/#authorization","title":"Authorization","text":"<p>Since Trino uses the Hive Metastore, and the Ranger plugin has already been installed on the Metastore, no additional authorization configuration is required. Trino will enforce the policies defined for the Hive tables. Accesses made from Trino are logged in the Ranger audit as the <code>hiveMetastore</code> application.</p>"},{"location":"useful-commands/","title":"Useful Commands","text":""},{"location":"useful-commands/#kerberos","title":"Kerberos","text":""},{"location":"useful-commands/#kinit","title":"kinit","text":"<p><pre><code>kinit -kt &lt;keytab&gt; &lt;principal&gt;\n</code></pre> Example: <pre><code>kinit -kt /etc/security/keytab/namenode.keytab hdfs/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG\n</code></pre></p>"},{"location":"useful-commands/#klist","title":"klist","text":"<p>Show current user kerberos ticket: <pre><code>klist\n</code></pre> Show existing kerberos keytab details: <pre><code>klist -kte &lt;keytab&gt;\n</code></pre></p>"},{"location":"useful-commands/#ktutil","title":"ktutil","text":"<p>Creating new keytab: <pre><code>ktutil\nadd_entry -password -p hdfs/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes128-cts-hmac-sha1-96\nadd_entry -password -p hdfs/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes256-cts-hmac-sha1-96\nadd_entry -password -p hdfs/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes128-cts-hmac-sha256-128\nadd_entry -password -p host/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes128-cts-hmac-sha1-96\nadd_entry -password -p host/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes256-cts-hmac-sha1-96\nadd_entry -password -p host/namenode.company.bigdata.svc.cluster.local@HOMELDAP.ORG -k 1 -e aes128-cts-hmac-sha256-128\nwkt ./files/namenode.keytab\n</code></pre></p>"},{"location":"useful-commands/#kubernetes","title":"Kubernetes","text":""},{"location":"useful-commands/#containerd-prune-unused-images","title":"Containerd prune unused images","text":"<pre><code>export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml\n/var/lib/rancher/rke2/bin/crictl rmi --prune\n</code></pre>"},{"location":"useful-commands/#linux","title":"Linux","text":""},{"location":"useful-commands/#disable-selinux","title":"Disable SELINUX","text":"<pre><code>sudo sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config &amp;&amp; sudo setenforce \n</code></pre>"},{"location":"useful-commands/#disable-firewalld","title":"Disable firewalld","text":"<pre><code>systemctl disable --now firewalld\n</code></pre>"},{"location":"useful-commands/#set-systemd-timeout","title":"Set systemd timeout","text":"<pre><code>sed -i '/^#DefaultTimeoutStopSec=/c\\DefaultTimeoutStopSec=10s' /etc/systemd/system.conf /etc/systemd/user.conf &amp;&amp; sudo systemctl daemon-reexec\n</code></pre>"},{"location":"useful-commands/#longhorn-packages","title":"Longhorn Packages","text":"<pre><code>dnf install nfs-utils iscsi-initiator-utils cryptsetup device-mapper\n</code></pre>"},{"location":"useful-commands/#extend-root-path-size-without-restartdevvda2","title":"Extend root path size without restart(/dev/vda2)","text":"<p><pre><code>qemu-img resize disk.qcow2 +10G\n</code></pre> <pre><code>parted /dev/vda\nresizepart 2 100%\n</code></pre> <pre><code>pvresize /dev/vda2\nlvdisplay\nlvextend -r -l +100%FREE /dev/mapper/rl-root\nlsblk\n</code></pre></p>"}]}